<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=edge"/>
<title>Test results - Class com.mesosphere.sdk.kafka.scheduler.ServiceTest</title>
<link href="../css/base-style.css" rel="stylesheet" type="text/css"/>
<link href="../css/style.css" rel="stylesheet" type="text/css"/>
<script src="../js/report.js" type="text/javascript"></script>
</head>
<body>
<div id="content">
<h1>Class com.mesosphere.sdk.kafka.scheduler.ServiceTest</h1>
<div class="breadcrumbs">
<a href="../index.html">all</a> &gt; 
<a href="../packages/com.mesosphere.sdk.kafka.scheduler.html">com.mesosphere.sdk.kafka.scheduler</a> &gt; ServiceTest</div>
<div id="summary">
<table>
<tr>
<td>
<div class="summaryGroup">
<table>
<tr>
<td>
<div class="infoBox" id="tests">
<div class="counter">5</div>
<p>tests</p>
</div>
</td>
<td>
<div class="infoBox" id="failures">
<div class="counter">0</div>
<p>failures</p>
</div>
</td>
<td>
<div class="infoBox" id="ignored">
<div class="counter">0</div>
<p>ignored</p>
</div>
</td>
<td>
<div class="infoBox" id="duration">
<div class="counter">3.243s</div>
<p>duration</p>
</div>
</td>
</tr>
</table>
</div>
</td>
<td>
<div class="infoBox success" id="successRate">
<div class="percent">100%</div>
<p>successful</p>
</div>
</td>
</tr>
</table>
</div>
<div id="tabs">
<ul class="tabLinks">
<li>
<a href="#tab0">Tests</a>
</li>
<li>
<a href="#tab1">Standard output</a>
</li>
</ul>
<div id="tab0" class="tab">
<h2>Tests</h2>
<table>
<thead>
<tr>
<th>Test</th>
<th>Duration</th>
<th>Result</th>
</tr>
</thead>
<tr>
<td class="success">allowRackChanges</td>
<td class="success">0.212s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">rejectRackDisablement</td>
<td class="success">2.752s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">rejectRackEnablement</td>
<td class="success">0.144s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testRegionAwareness</td>
<td class="success">0.048s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testSpec</td>
<td class="success">0.087s</td>
<td class="success">passed</td>
</tr>
</table>
</div>
<div id="tab1" class="tab">
<h2>Standard output</h2>
<span class="code">
<pre>INFO  2019-09-05 09:55:43,591 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
  kafka:
    count: 3
    placement: '[[&quot;@zone&quot;, &quot;GROUP_BY&quot;, &quot;3&quot;]]'
    uris:
      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
      - https://test-url/jre.tgz
      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
      - https://test-url/libmesos-bundle.tgz
      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
      - https://test-url/artifacts/setup-helper.zip
      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
          KAFKA_DISK_PATH: &quot;kafka-broker-data&quot;
          KAFKA_HEAP_OPTS: &quot;-Xms512M -Xmx512M&quot;
          JAVA_HOME: &quot;$MESOS_SANDBOX/jdk*/&quot;
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

          # setup-helper determines the correct listeners and security.inter.broker.protocol.
          # it relies on the task IP being stored in MESOS_CONTAINER_IP
          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
          export SETUP_HELPER_LISTENERS=`cat listeners`
          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
          export SETUP_HELPER_SUPER_USERS=`cat super.users`

          ./bootstrap -resolve=false

          # NOTE: We add some custom statsd libraries for statsd metrics as well
          # as a custom zookeeper library to support our own ZK running
          # kerberized. The custom zk library does not do DNS reverse resolution
          # of the ZK hostnames.
          #
          # Additionally, we include a custom principal builder
          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Clean up any pre-existing zookeeper library
          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
            template: server.properties.mustache
            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
            # The broker has started when it logs a specific &quot;started&quot; log line. An example is below:
            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

            echo &quot;Checking for started log line in $kafka_server_log_files.&quot;
            grep -q &quot;INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)&quot; $kafka_server_log_files
            if [ $? -eq 0 ] ; then
              echo &quot;Found started log line.&quot;
            else
              echo &quot;started log line not found. Exiting.&quot;
              exit 1
            fi
            echo &quot;Required log line found. Broker is ready.&quot;
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:
  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

INFO  2019-09-05 09:55:43,738 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@40633e95[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=&lt;null&gt;]
INFO  2019-09-05 09:55:43,789 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@zone, GROUP_BY, 3]' resulted in placement rule: 'RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}'
INFO  2019-09-05 09:55:43,799 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
INFO  2019-09-05 09:55:44,093 [Test worker] SchedulerBuilder:getDefaultScheduler(510): Unable to retrieve last configuration. Assuming that no prior deployment has completed
INFO  2019-09-05 09:55:44,099 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 12 validators...
INFO  2019-09-05 09:55:44,103 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;RoundRobinByZoneRule&quot;,
        &quot;zone-count&quot; : 3,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:44,112 [Test worker] DefaultConfigurationUpdater:updateConfiguration(147): Skipping config diff: There is no old config target to diff against
INFO  2019-09-05 09:55:44,120 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'null' is different from new configuration '531e4dea-74ea-4592-b428-183f90e325aa'. 
INFO  2019-09-05 09:55:44,121 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
INFO  2019-09-05 09:55:44,121 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 531e4dea-74ea-4592-b428-183f90e325aa: OK
INFO  2019-09-05 09:55:44,122 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
INFO  2019-09-05 09:55:44,130 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:44,139 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:44,142 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:44,143 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,143 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,144 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:44,144 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:44,145 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:44,145 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,145 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,145 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:44,146 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:44,146 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:44,146 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,147 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,149 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:44,150 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
INFO  2019-09-05 09:55:44,158 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,165 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
INFO  2019-09-05 09:55:44,165 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
INFO  2019-09-05 09:55:44,190 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
INFO  2019-09-05 09:55:44,197 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
INFO  2019-09-05 09:55:44,526 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
  kafka:
    count: 3
    placement: '[[&quot;@hostname&quot;, &quot;UNIQUE&quot;]]'
    uris:
      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
      - https://test-url/jre.tgz
      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
      - https://test-url/libmesos-bundle.tgz
      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
      - https://test-url/artifacts/setup-helper.zip
      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
          KAFKA_DISK_PATH: &quot;kafka-broker-data&quot;
          KAFKA_HEAP_OPTS: &quot;-Xms512M -Xmx512M&quot;
          JAVA_HOME: &quot;$MESOS_SANDBOX/jdk*/&quot;
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

          # setup-helper determines the correct listeners and security.inter.broker.protocol.
          # it relies on the task IP being stored in MESOS_CONTAINER_IP
          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
          export SETUP_HELPER_LISTENERS=`cat listeners`
          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
          export SETUP_HELPER_SUPER_USERS=`cat super.users`

          ./bootstrap -resolve=false

          # NOTE: We add some custom statsd libraries for statsd metrics as well
          # as a custom zookeeper library to support our own ZK running
          # kerberized. The custom zk library does not do DNS reverse resolution
          # of the ZK hostnames.
          #
          # Additionally, we include a custom principal builder
          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Clean up any pre-existing zookeeper library
          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
            template: server.properties.mustache
            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
            # The broker has started when it logs a specific &quot;started&quot; log line. An example is below:
            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

            echo &quot;Checking for started log line in $kafka_server_log_files.&quot;
            grep -q &quot;INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)&quot; $kafka_server_log_files
            if [ $? -eq 0 ] ; then
              echo &quot;Found started log line.&quot;
            else
              echo &quot;started log line not found. Exiting.&quot;
              exit 1
            fi
            echo &quot;Required log line found. Broker is ready.&quot;
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:
  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

INFO  2019-09-05 09:55:44,531 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@6891c3a[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=&lt;null&gt;]
INFO  2019-09-05 09:55:44,534 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@hostname, UNIQUE]' resulted in placement rule: 'MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}'
INFO  2019-09-05 09:55:44,535 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
INFO  2019-09-05 09:55:44,563 [Test worker] ConfigStore:fetch(168): Fetching configuration with ID=531e4dea-74ea-4592-b428-183f90e325aa from Configurations/531e4dea-74ea-4592-b428-183f90e325aa
INFO  2019-09-05 09:55:44,566 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:44,567 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:44,567 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:44,568 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,568 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,568 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:44,569 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:44,569 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:44,569 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,570 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,570 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:44,570 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:44,571 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:44,571 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,571 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,571 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:44,573 [Test worker] SchedulerBuilder:getDefaultScheduler(497): Previous deploy plan state: Plan: deploy (PENDING)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,574 [Test worker] SchedulerBuilder:getDefaultScheduler(503): Deployment has not previously completed
INFO  2019-09-05 09:55:44,574 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 13 validators...
INFO  2019-09-05 09:55:44,574 [Test worker] DefaultConfigurationUpdater:updateConfiguration(123): Loading current target configuration: 531e4dea-74ea-4592-b428-183f90e325aa
INFO  2019-09-05 09:55:44,576 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;MaxPerHostnameRule&quot;,
        &quot;max&quot; : 1,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:44,583 [Test worker] DefaultConfigurationUpdater:updateConfiguration(151): Prior target config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;RoundRobinByZoneRule&quot;,
        &quot;zone-count&quot; : 3,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:44,599 [Test worker] DefaultConfigurationUpdater:printConfigDiff(330): Difference between configs:
--- ServiceSpec.old
+++ ServiceSpec.new
@@ -233,6 +233,6 @@
         &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
       }, {
-        &quot;@type&quot; : &quot;RoundRobinByZoneRule&quot;,
-        &quot;zone-count&quot; : 3,
+        &quot;@type&quot; : &quot;MaxPerHostnameRule&quot;,
+        &quot;max&quot; : 1,
         &quot;task-filter&quot; : {
           &quot;@type&quot; : &quot;RegexMatcher&quot;,
WARN  2019-09-05 09:55:44,607 [Test worker] DefaultConfigurationUpdater:updateConfiguration(173): New configuration failed validation against current target configuration 531e4dea-74ea-4592-b428-183f90e325aa, with 1 errors across 13 validators:
1: Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' =&gt; 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false
INFO  2019-09-05 09:55:44,608 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
INFO  2019-09-05 09:55:44,608 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 531e4dea-74ea-4592-b428-183f90e325aa: OK
INFO  2019-09-05 09:55:44,608 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
WARN  2019-09-05 09:55:44,608 [Test worker] SchedulerBuilder:getDefaultScheduler(522): Failed to update configuration due to validation errors: [Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' =&gt; 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false]
INFO  2019-09-05 09:55:44,609 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:44,609 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:44,610 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:44,610 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,610 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,610 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:44,611 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:44,611 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:44,611 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,611 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,612 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:44,612 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:44,612 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:44,613 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,613 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,613 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:44,613 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
INFO  2019-09-05 09:55:44,615 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (ERROR)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
Errors:
  Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' =&gt; 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false
INFO  2019-09-05 09:55:44,615 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
INFO  2019-09-05 09:55:44,616 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
INFO  2019-09-05 09:55:44,617 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
INFO  2019-09-05 09:55:44,618 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
INFO  2019-09-05 09:55:44,622 [Test worker] ServiceTestRunner:run(383): SEND:   Framework registration completed
INFO  2019-09-05 09:55:44,628 [Test worker] FrameworkScheduler:registered(163): Registered framework with frameworkId: test-framework-id
INFO  2019-09-05 09:55:44,635 [Test worker] ExplicitReconciler:start(110): Added 0 unreconciled tasks to reconciler: 0 tasks to reconcile: []
INFO  2019-09-05 09:55:44,635 [Test worker] ExplicitReconciler:reconcile(182): Completed explicit reconciliation
INFO  2019-09-05 09:55:44,636 [Test worker] ImplicitReconciler:lambda$static$0(38): Triggering implicit reconciliation
INFO  2019-09-05 09:55:44,637 [Test worker] ServiceTestRunner:run(376): EXPECT: Plan deploy has status ERROR
INFO  2019-09-05 09:55:44,680 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
  kafka:
    count: 3
    placement: '[[&quot;hostname&quot;, &quot;MAX_PER&quot;, &quot;1&quot;]]'
    uris:
      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
      - https://test-url/jre.tgz
      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
      - https://test-url/libmesos-bundle.tgz
      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
      - https://test-url/artifacts/setup-helper.zip
      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
          KAFKA_DISK_PATH: &quot;kafka-broker-data&quot;
          KAFKA_HEAP_OPTS: &quot;-Xms512M -Xmx512M&quot;
          JAVA_HOME: &quot;$MESOS_SANDBOX/jdk*/&quot;
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

          # setup-helper determines the correct listeners and security.inter.broker.protocol.
          # it relies on the task IP being stored in MESOS_CONTAINER_IP
          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
          export SETUP_HELPER_LISTENERS=`cat listeners`
          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
          export SETUP_HELPER_SUPER_USERS=`cat super.users`

          ./bootstrap -resolve=false

          # NOTE: We add some custom statsd libraries for statsd metrics as well
          # as a custom zookeeper library to support our own ZK running
          # kerberized. The custom zk library does not do DNS reverse resolution
          # of the ZK hostnames.
          #
          # Additionally, we include a custom principal builder
          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Clean up any pre-existing zookeeper library
          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
            template: server.properties.mustache
            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
            # The broker has started when it logs a specific &quot;started&quot; log line. An example is below:
            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

            echo &quot;Checking for started log line in $kafka_server_log_files.&quot;
            grep -q &quot;INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)&quot; $kafka_server_log_files
            if [ $? -eq 0 ] ; then
              echo &quot;Found started log line.&quot;
            else
              echo &quot;started log line not found. Exiting.&quot;
              exit 1
            fi
            echo &quot;Required log line found. Broker is ready.&quot;
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:
  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

INFO  2019-09-05 09:55:44,684 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@5902b63f[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=&lt;null&gt;]
INFO  2019-09-05 09:55:44,686 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[hostname, MAX_PER, 1]' resulted in placement rule: 'MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}'
INFO  2019-09-05 09:55:44,686 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
INFO  2019-09-05 09:55:44,709 [Test worker] SchedulerBuilder:getDefaultScheduler(510): Unable to retrieve last configuration. Assuming that no prior deployment has completed
INFO  2019-09-05 09:55:44,709 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 12 validators...
INFO  2019-09-05 09:55:44,710 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;MaxPerHostnameRule&quot;,
        &quot;max&quot; : 1,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:44,712 [Test worker] DefaultConfigurationUpdater:updateConfiguration(147): Skipping config diff: There is no old config target to diff against
INFO  2019-09-05 09:55:44,714 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'null' is different from new configuration 'e3544347-4562-461b-a5e3-ae49b9027d3f'. 
INFO  2019-09-05 09:55:44,715 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
INFO  2019-09-05 09:55:44,715 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - e3544347-4562-461b-a5e3-ae49b9027d3f: OK
INFO  2019-09-05 09:55:44,715 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
INFO  2019-09-05 09:55:44,715 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:44,716 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:44,716 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:44,716 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,716 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,717 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:44,717 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:44,717 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:44,717 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,718 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,718 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:44,718 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:44,718 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:44,719 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,719 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,719 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:44,719 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
INFO  2019-09-05 09:55:44,720 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,720 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
INFO  2019-09-05 09:55:44,721 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
INFO  2019-09-05 09:55:44,722 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
INFO  2019-09-05 09:55:44,723 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
INFO  2019-09-05 09:55:44,762 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
  kafka:
    count: 3
    placement: '[[&quot;@zone&quot;, &quot;MAX_PER&quot;, &quot;3&quot;]]'
    uris:
      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
      - https://test-url/jre.tgz
      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
      - https://test-url/libmesos-bundle.tgz
      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
      - https://test-url/artifacts/setup-helper.zip
      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
          KAFKA_DISK_PATH: &quot;kafka-broker-data&quot;
          KAFKA_HEAP_OPTS: &quot;-Xms512M -Xmx512M&quot;
          JAVA_HOME: &quot;$MESOS_SANDBOX/jdk*/&quot;
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

          # setup-helper determines the correct listeners and security.inter.broker.protocol.
          # it relies on the task IP being stored in MESOS_CONTAINER_IP
          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
          export SETUP_HELPER_LISTENERS=`cat listeners`
          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
          export SETUP_HELPER_SUPER_USERS=`cat super.users`

          ./bootstrap -resolve=false

          # NOTE: We add some custom statsd libraries for statsd metrics as well
          # as a custom zookeeper library to support our own ZK running
          # kerberized. The custom zk library does not do DNS reverse resolution
          # of the ZK hostnames.
          #
          # Additionally, we include a custom principal builder
          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Clean up any pre-existing zookeeper library
          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
            template: server.properties.mustache
            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
            # The broker has started when it logs a specific &quot;started&quot; log line. An example is below:
            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

            echo &quot;Checking for started log line in $kafka_server_log_files.&quot;
            grep -q &quot;INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)&quot; $kafka_server_log_files
            if [ $? -eq 0 ] ; then
              echo &quot;Found started log line.&quot;
            else
              echo &quot;started log line not found. Exiting.&quot;
              exit 1
            fi
            echo &quot;Required log line found. Broker is ready.&quot;
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:
  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

INFO  2019-09-05 09:55:44,766 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@2c884f5a[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=&lt;null&gt;]
INFO  2019-09-05 09:55:44,768 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@zone, MAX_PER, 3]' resulted in placement rule: 'com.mesosphere.sdk.offer.evaluate.placement.MaxPerZoneRule@2a79791b'
INFO  2019-09-05 09:55:44,769 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
INFO  2019-09-05 09:55:44,792 [Test worker] SchedulerBuilder:getDefaultScheduler(510): Unable to retrieve last configuration. Assuming that no prior deployment has completed
INFO  2019-09-05 09:55:44,793 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 12 validators...
INFO  2019-09-05 09:55:44,795 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;MaxPerZoneRule&quot;,
        &quot;max&quot; : 3,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:44,797 [Test worker] DefaultConfigurationUpdater:updateConfiguration(147): Skipping config diff: There is no old config target to diff against
INFO  2019-09-05 09:55:44,800 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'null' is different from new configuration 'bfc361b7-b0b3-4082-a9e8-9d5da93c801a'. 
INFO  2019-09-05 09:55:44,801 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
INFO  2019-09-05 09:55:44,801 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - bfc361b7-b0b3-4082-a9e8-9d5da93c801a: OK
INFO  2019-09-05 09:55:44,801 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
INFO  2019-09-05 09:55:44,802 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:44,802 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:44,802 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:44,803 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,803 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,803 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:44,803 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:44,804 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:44,804 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,804 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,804 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:44,805 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:44,805 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:44,805 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,806 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,806 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:44,806 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
INFO  2019-09-05 09:55:44,807 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,808 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
INFO  2019-09-05 09:55:44,808 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
INFO  2019-09-05 09:55:44,809 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
INFO  2019-09-05 09:55:44,810 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
INFO  2019-09-05 09:55:44,862 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
  kafka:
    count: 3
    placement: '[[&quot;@zone&quot;, &quot;GROUP_BY&quot;, &quot;3&quot;]]'
    uris:
      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
      - https://test-url/jre.tgz
      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
      - https://test-url/libmesos-bundle.tgz
      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
      - https://test-url/artifacts/setup-helper.zip
      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
          KAFKA_DISK_PATH: &quot;kafka-broker-data&quot;
          KAFKA_HEAP_OPTS: &quot;-Xms512M -Xmx512M&quot;
          JAVA_HOME: &quot;$MESOS_SANDBOX/jdk*/&quot;
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

          # setup-helper determines the correct listeners and security.inter.broker.protocol.
          # it relies on the task IP being stored in MESOS_CONTAINER_IP
          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
          export SETUP_HELPER_LISTENERS=`cat listeners`
          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
          export SETUP_HELPER_SUPER_USERS=`cat super.users`

          ./bootstrap -resolve=false

          # NOTE: We add some custom statsd libraries for statsd metrics as well
          # as a custom zookeeper library to support our own ZK running
          # kerberized. The custom zk library does not do DNS reverse resolution
          # of the ZK hostnames.
          #
          # Additionally, we include a custom principal builder
          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Clean up any pre-existing zookeeper library
          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
            template: server.properties.mustache
            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
            # The broker has started when it logs a specific &quot;started&quot; log line. An example is below:
            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

            echo &quot;Checking for started log line in $kafka_server_log_files.&quot;
            grep -q &quot;INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)&quot; $kafka_server_log_files
            if [ $? -eq 0 ] ; then
              echo &quot;Found started log line.&quot;
            else
              echo &quot;started log line not found. Exiting.&quot;
              exit 1
            fi
            echo &quot;Required log line found. Broker is ready.&quot;
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:
  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

INFO  2019-09-05 09:55:44,866 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@1bccf2f[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=&lt;null&gt;]
INFO  2019-09-05 09:55:44,868 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@zone, GROUP_BY, 3]' resulted in placement rule: 'RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}'
INFO  2019-09-05 09:55:44,869 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
INFO  2019-09-05 09:55:44,890 [Test worker] ConfigStore:fetch(168): Fetching configuration with ID=bfc361b7-b0b3-4082-a9e8-9d5da93c801a from Configurations/bfc361b7-b0b3-4082-a9e8-9d5da93c801a
INFO  2019-09-05 09:55:44,893 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:44,894 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:44,894 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:44,894 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,894 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,895 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:44,895 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:44,895 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:44,896 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,896 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,896 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:44,897 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:44,897 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:44,897 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,897 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,898 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:44,898 [Test worker] SchedulerBuilder:getDefaultScheduler(497): Previous deploy plan state: Plan: deploy (PENDING)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,899 [Test worker] SchedulerBuilder:getDefaultScheduler(503): Deployment has not previously completed
INFO  2019-09-05 09:55:44,899 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 13 validators...
INFO  2019-09-05 09:55:44,900 [Test worker] DefaultConfigurationUpdater:updateConfiguration(123): Loading current target configuration: bfc361b7-b0b3-4082-a9e8-9d5da93c801a
INFO  2019-09-05 09:55:44,902 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;RoundRobinByZoneRule&quot;,
        &quot;zone-count&quot; : 3,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:44,916 [Test worker] DefaultConfigurationUpdater:updateConfiguration(151): Prior target config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;MaxPerZoneRule&quot;,
        &quot;max&quot; : 3,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:44,920 [Test worker] DefaultConfigurationUpdater:printConfigDiff(330): Difference between configs:
--- ServiceSpec.old
+++ ServiceSpec.new
@@ -233,6 +233,6 @@
         &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
       }, {
-        &quot;@type&quot; : &quot;MaxPerZoneRule&quot;,
-        &quot;max&quot; : 3,
+        &quot;@type&quot; : &quot;RoundRobinByZoneRule&quot;,
+        &quot;zone-count&quot; : 3,
         &quot;task-filter&quot; : {
           &quot;@type&quot; : &quot;RegexMatcher&quot;,
INFO  2019-09-05 09:55:44,923 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'bfc361b7-b0b3-4082-a9e8-9d5da93c801a' is different from new configuration '9167fa5b-28b9-46b0-bfa7-305c6f3d696b'. 
INFO  2019-09-05 09:55:44,924 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 2 listed configurations before cleanup:
INFO  2019-09-05 09:55:44,924 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 9167fa5b-28b9-46b0-bfa7-305c6f3d696b: OK
INFO  2019-09-05 09:55:44,924 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - bfc361b7-b0b3-4082-a9e8-9d5da93c801a: OK
INFO  2019-09-05 09:55:44,925 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 1 unused configs: [bfc361b7-b0b3-4082-a9e8-9d5da93c801a]
INFO  2019-09-05 09:55:44,925 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:44,925 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:44,926 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:44,926 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,926 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,927 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:44,927 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:44,927 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:44,927 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,928 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,928 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:44,928 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:44,929 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:44,929 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,929 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:44,929 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:44,929 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
INFO  2019-09-05 09:55:44,930 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,931 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
INFO  2019-09-05 09:55:44,931 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
INFO  2019-09-05 09:55:44,932 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
INFO  2019-09-05 09:55:44,933 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
INFO  2019-09-05 09:55:44,936 [Test worker] ServiceTestRunner:run(383): SEND:   Framework registration completed
INFO  2019-09-05 09:55:44,936 [Test worker] FrameworkScheduler:registered(163): Registered framework with frameworkId: test-framework-id
INFO  2019-09-05 09:55:44,937 [Test worker] ExplicitReconciler:start(110): Added 0 unreconciled tasks to reconciler: 0 tasks to reconcile: []
INFO  2019-09-05 09:55:44,937 [Test worker] ExplicitReconciler:reconcile(182): Completed explicit reconciliation
INFO  2019-09-05 09:55:44,937 [Test worker] ImplicitReconciler:lambda$static$0(38): Triggering implicit reconciliation
INFO  2019-09-05 09:55:44,937 [Test worker] ServiceTestRunner:run(376): EXPECT: Deploy plan does not have ERROR as status.
INFO  2019-09-05 09:55:44,977 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
  kafka:
    count: 3
    placement: '[[&quot;@hostname&quot;, &quot;UNIQUE&quot;]]'
    uris:
      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
      - https://test-url/jre.tgz
      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
      - https://test-url/libmesos-bundle.tgz
      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
      - https://test-url/artifacts/setup-helper.zip
      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
          KAFKA_DISK_PATH: &quot;kafka-broker-data&quot;
          KAFKA_HEAP_OPTS: &quot;-Xms512M -Xmx512M&quot;
          JAVA_HOME: &quot;$MESOS_SANDBOX/jdk*/&quot;
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

          # setup-helper determines the correct listeners and security.inter.broker.protocol.
          # it relies on the task IP being stored in MESOS_CONTAINER_IP
          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
          export SETUP_HELPER_LISTENERS=`cat listeners`
          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
          export SETUP_HELPER_SUPER_USERS=`cat super.users`

          ./bootstrap -resolve=false

          # NOTE: We add some custom statsd libraries for statsd metrics as well
          # as a custom zookeeper library to support our own ZK running
          # kerberized. The custom zk library does not do DNS reverse resolution
          # of the ZK hostnames.
          #
          # Additionally, we include a custom principal builder
          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Clean up any pre-existing zookeeper library
          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
            template: server.properties.mustache
            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
            # The broker has started when it logs a specific &quot;started&quot; log line. An example is below:
            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

            echo &quot;Checking for started log line in $kafka_server_log_files.&quot;
            grep -q &quot;INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)&quot; $kafka_server_log_files
            if [ $? -eq 0 ] ; then
              echo &quot;Found started log line.&quot;
            else
              echo &quot;started log line not found. Exiting.&quot;
              exit 1
            fi
            echo &quot;Required log line found. Broker is ready.&quot;
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:
  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

INFO  2019-09-05 09:55:44,980 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@547766fd[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=&lt;null&gt;]
INFO  2019-09-05 09:55:44,982 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@hostname, UNIQUE]' resulted in placement rule: 'MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}'
INFO  2019-09-05 09:55:44,983 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
INFO  2019-09-05 09:55:45,000 [Test worker] SchedulerBuilder:getDefaultScheduler(510): Unable to retrieve last configuration. Assuming that no prior deployment has completed
INFO  2019-09-05 09:55:45,000 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 12 validators...
INFO  2019-09-05 09:55:45,001 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;MaxPerHostnameRule&quot;,
        &quot;max&quot; : 1,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:45,002 [Test worker] DefaultConfigurationUpdater:updateConfiguration(147): Skipping config diff: There is no old config target to diff against
INFO  2019-09-05 09:55:45,005 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'null' is different from new configuration '988703c5-d9dc-4002-9100-78476fd71a2a'. 
INFO  2019-09-05 09:55:45,005 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
INFO  2019-09-05 09:55:45,005 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 988703c5-d9dc-4002-9100-78476fd71a2a: OK
INFO  2019-09-05 09:55:45,005 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
INFO  2019-09-05 09:55:45,006 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:45,006 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:45,006 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:45,006 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,007 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,007 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:45,007 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:45,008 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:45,008 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,008 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,008 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:45,009 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:45,009 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:45,009 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,009 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,010 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:45,010 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
INFO  2019-09-05 09:55:45,010 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:45,011 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
INFO  2019-09-05 09:55:45,011 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
INFO  2019-09-05 09:55:45,012 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
INFO  2019-09-05 09:55:45,013 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
INFO  2019-09-05 09:55:45,032 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
  kafka:
    count: 3
    placement: '[[&quot;@zone&quot;, &quot;GROUP_BY&quot;, &quot;3&quot;]]'
    uris:
      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
      - https://test-url/jre.tgz
      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
      - https://test-url/libmesos-bundle.tgz
      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
      - https://test-url/artifacts/setup-helper.zip
      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
          KAFKA_DISK_PATH: &quot;kafka-broker-data&quot;
          KAFKA_HEAP_OPTS: &quot;-Xms512M -Xmx512M&quot;
          JAVA_HOME: &quot;$MESOS_SANDBOX/jdk*/&quot;
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

          # setup-helper determines the correct listeners and security.inter.broker.protocol.
          # it relies on the task IP being stored in MESOS_CONTAINER_IP
          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
          export SETUP_HELPER_LISTENERS=`cat listeners`
          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
          export SETUP_HELPER_SUPER_USERS=`cat super.users`

          ./bootstrap -resolve=false

          # NOTE: We add some custom statsd libraries for statsd metrics as well
          # as a custom zookeeper library to support our own ZK running
          # kerberized. The custom zk library does not do DNS reverse resolution
          # of the ZK hostnames.
          #
          # Additionally, we include a custom principal builder
          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Clean up any pre-existing zookeeper library
          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
            template: server.properties.mustache
            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
            # The broker has started when it logs a specific &quot;started&quot; log line. An example is below:
            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

            echo &quot;Checking for started log line in $kafka_server_log_files.&quot;
            grep -q &quot;INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)&quot; $kafka_server_log_files
            if [ $? -eq 0 ] ; then
              echo &quot;Found started log line.&quot;
            else
              echo &quot;started log line not found. Exiting.&quot;
              exit 1
            fi
            echo &quot;Required log line found. Broker is ready.&quot;
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:
  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

INFO  2019-09-05 09:55:45,035 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@22a6b4e3[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=&lt;null&gt;]
INFO  2019-09-05 09:55:45,036 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@zone, GROUP_BY, 3]' resulted in placement rule: 'RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}'
INFO  2019-09-05 09:55:45,037 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
INFO  2019-09-05 09:55:45,053 [Test worker] ConfigStore:fetch(168): Fetching configuration with ID=988703c5-d9dc-4002-9100-78476fd71a2a from Configurations/988703c5-d9dc-4002-9100-78476fd71a2a
INFO  2019-09-05 09:55:45,055 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:45,055 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:45,056 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:45,056 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,056 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,056 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:45,057 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:45,057 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:45,057 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,057 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,058 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:45,058 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:45,058 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:45,058 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,059 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,059 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:45,059 [Test worker] SchedulerBuilder:getDefaultScheduler(497): Previous deploy plan state: Plan: deploy (PENDING)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:45,060 [Test worker] SchedulerBuilder:getDefaultScheduler(503): Deployment has not previously completed
INFO  2019-09-05 09:55:45,060 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 13 validators...
INFO  2019-09-05 09:55:45,060 [Test worker] DefaultConfigurationUpdater:updateConfiguration(123): Loading current target configuration: 988703c5-d9dc-4002-9100-78476fd71a2a
INFO  2019-09-05 09:55:45,061 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;RoundRobinByZoneRule&quot;,
        &quot;zone-count&quot; : 3,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:45,063 [Test worker] DefaultConfigurationUpdater:updateConfiguration(151): Prior target config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;MaxPerHostnameRule&quot;,
        &quot;max&quot; : 1,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:45,065 [Test worker] DefaultConfigurationUpdater:printConfigDiff(330): Difference between configs:
--- ServiceSpec.old
+++ ServiceSpec.new
@@ -233,6 +233,6 @@
         &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
       }, {
-        &quot;@type&quot; : &quot;MaxPerHostnameRule&quot;,
-        &quot;max&quot; : 1,
+        &quot;@type&quot; : &quot;RoundRobinByZoneRule&quot;,
+        &quot;zone-count&quot; : 3,
         &quot;task-filter&quot; : {
           &quot;@type&quot; : &quot;RegexMatcher&quot;,
WARN  2019-09-05 09:55:45,067 [Test worker] DefaultConfigurationUpdater:updateConfiguration(173): New configuration failed validation against current target configuration 988703c5-d9dc-4002-9100-78476fd71a2a, with 1 errors across 13 validators:
1: Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' =&gt; 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false
INFO  2019-09-05 09:55:45,067 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
INFO  2019-09-05 09:55:45,068 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 988703c5-d9dc-4002-9100-78476fd71a2a: OK
INFO  2019-09-05 09:55:45,068 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
WARN  2019-09-05 09:55:45,068 [Test worker] SchedulerBuilder:getDefaultScheduler(522): Failed to update configuration due to validation errors: [Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' =&gt; 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false]
INFO  2019-09-05 09:55:45,068 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:45,069 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:45,069 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:45,069 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,070 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,070 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:45,070 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:45,071 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:45,071 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,071 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,071 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:45,072 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:45,072 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:45,072 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,073 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,073 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:45,073 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
INFO  2019-09-05 09:55:45,074 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (ERROR)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
Errors:
  Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' =&gt; 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false
INFO  2019-09-05 09:55:45,074 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
INFO  2019-09-05 09:55:45,075 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
INFO  2019-09-05 09:55:45,076 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
INFO  2019-09-05 09:55:45,076 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
INFO  2019-09-05 09:55:45,079 [Test worker] ServiceTestRunner:run(383): SEND:   Framework registration completed
INFO  2019-09-05 09:55:45,079 [Test worker] FrameworkScheduler:registered(163): Registered framework with frameworkId: test-framework-id
INFO  2019-09-05 09:55:45,080 [Test worker] ExplicitReconciler:start(110): Added 0 unreconciled tasks to reconciler: 0 tasks to reconcile: []
INFO  2019-09-05 09:55:45,081 [Test worker] ExplicitReconciler:reconcile(182): Completed explicit reconciliation
INFO  2019-09-05 09:55:45,081 [Test worker] ImplicitReconciler:lambda$static$0(38): Triggering implicit reconciliation
INFO  2019-09-05 09:55:45,081 [Test worker] ServiceTestRunner:run(376): EXPECT: Plan deploy has status ERROR
INFO  2019-09-05 09:55:45,099 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
  kafka:
    count: 3
    placement: '[[&quot;hostname&quot;, &quot;MAX_PER&quot;, &quot;1&quot;]]'
    uris:
      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
      - https://test-url/jre.tgz
      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
      - https://test-url/libmesos-bundle.tgz
      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
      - https://test-url/artifacts/setup-helper.zip
      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
          KAFKA_DISK_PATH: &quot;kafka-broker-data&quot;
          KAFKA_HEAP_OPTS: &quot;-Xms512M -Xmx512M&quot;
          JAVA_HOME: &quot;$MESOS_SANDBOX/jdk*/&quot;
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

          # setup-helper determines the correct listeners and security.inter.broker.protocol.
          # it relies on the task IP being stored in MESOS_CONTAINER_IP
          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
          export SETUP_HELPER_LISTENERS=`cat listeners`
          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
          export SETUP_HELPER_SUPER_USERS=`cat super.users`

          ./bootstrap -resolve=false

          # NOTE: We add some custom statsd libraries for statsd metrics as well
          # as a custom zookeeper library to support our own ZK running
          # kerberized. The custom zk library does not do DNS reverse resolution
          # of the ZK hostnames.
          #
          # Additionally, we include a custom principal builder
          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Clean up any pre-existing zookeeper library
          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
            template: server.properties.mustache
            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
            # The broker has started when it logs a specific &quot;started&quot; log line. An example is below:
            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

            echo &quot;Checking for started log line in $kafka_server_log_files.&quot;
            grep -q &quot;INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)&quot; $kafka_server_log_files
            if [ $? -eq 0 ] ; then
              echo &quot;Found started log line.&quot;
            else
              echo &quot;started log line not found. Exiting.&quot;
              exit 1
            fi
            echo &quot;Required log line found. Broker is ready.&quot;
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:
  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

INFO  2019-09-05 09:55:45,101 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@29f8d30f[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=&lt;null&gt;]
INFO  2019-09-05 09:55:45,102 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[hostname, MAX_PER, 1]' resulted in placement rule: 'MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}'
INFO  2019-09-05 09:55:45,103 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
INFO  2019-09-05 09:55:45,118 [Test worker] SchedulerBuilder:getDefaultScheduler(510): Unable to retrieve last configuration. Assuming that no prior deployment has completed
INFO  2019-09-05 09:55:45,118 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 12 validators...
INFO  2019-09-05 09:55:45,119 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  &quot;name&quot; : &quot;kafka&quot;,
  &quot;role&quot; : &quot;kafka-role&quot;,
  &quot;principal&quot; : &quot;kafka-principal&quot;,
  &quot;user&quot; : &quot;nobody&quot;,
  &quot;goal&quot; : &quot;RUNNING&quot;,
  &quot;region&quot; : &quot;test-scheduler-region&quot;,
  &quot;web-url&quot; : null,
  &quot;zookeeper&quot; : &quot;master.mesos:2181&quot;,
  &quot;replacement-failure-policy&quot; : null,
  &quot;pod-specs&quot; : [ {
    &quot;type&quot; : &quot;kafka&quot;,
    &quot;user&quot; : &quot;nobody&quot;,
    &quot;count&quot; : 3,
    &quot;allow-decommission&quot; : false,
    &quot;image&quot; : null,
    &quot;networks&quot; : [ ],
    &quot;rlimits&quot; : [ {
      &quot;name&quot; : &quot;RLIMIT_NOFILE&quot;,
      &quot;soft&quot; : 128000,
      &quot;hard&quot; : 128000
    } ],
    &quot;uris&quot; : [ &quot;https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz&quot;, &quot;https://test-url/jre.tgz&quot;, &quot;https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip&quot;, &quot;https://test-url/libmesos-bundle.tgz&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar&quot;, &quot;https://test-url/artifacts/setup-helper.zip&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar&quot;, &quot;https://downloads.mesosphere.com/kafka/assets/nc64&quot; ],
    &quot;task-specs&quot; : [ {
      &quot;name&quot; : &quot;broker&quot;,
      &quot;goal&quot; : &quot;RUNNING&quot;,
      &quot;essential&quot; : true,
      &quot;resource-set&quot; : {
        &quot;id&quot; : &quot;broker-resource-set&quot;,
        &quot;resource-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;cpus&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 1.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;DefaultResourceSpec&quot;,
          &quot;name&quot; : &quot;mem&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 2048.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        }, {
          &quot;@type&quot; : &quot;NamedVIPSpec&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;RANGES&quot;,
            &quot;ranges&quot; : {
              &quot;range&quot; : [ {
                &quot;begin&quot; : 0,
                &quot;end&quot; : 0
              } ]
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;,
          &quot;env-key&quot; : &quot;KAFKA_BROKER_PORT&quot;,
          &quot;port-name&quot; : &quot;broker&quot;,
          &quot;visibility&quot; : &quot;EXTERNAL&quot;,
          &quot;network-names&quot; : [ ],
          &quot;protocol&quot; : &quot;tcp&quot;,
          &quot;vip-name&quot; : &quot;broker&quot;,
          &quot;vip-port&quot; : 9092,
          &quot;name&quot; : &quot;ports&quot;,
          &quot;ranges&quot; : [ ]
        } ],
        &quot;volume-specifications&quot; : [ {
          &quot;@type&quot; : &quot;DefaultVolumeSpec&quot;,
          &quot;type&quot; : &quot;ROOT&quot;,
          &quot;container-path&quot; : &quot;kafka-broker-data&quot;,
          &quot;profiles&quot; : [ ],
          &quot;name&quot; : &quot;disk&quot;,
          &quot;value&quot; : {
            &quot;type&quot; : &quot;SCALAR&quot;,
            &quot;scalar&quot; : {
              &quot;value&quot; : 5000.0
            }
          },
          &quot;role&quot; : &quot;kafka-role&quot;,
          &quot;pre-reserved-role&quot; : &quot;*&quot;,
          &quot;principal&quot; : &quot;kafka-principal&quot;
        } ],
        &quot;role&quot; : &quot;kafka-role&quot;,
        &quot;principal&quot; : &quot;kafka-principal&quot;
      },
      &quot;command-spec&quot; : {
        &quot;value&quot; : &quot;# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n&quot;,
        &quot;environment&quot; : {
          &quot;JAVA_HOME&quot; : &quot;$MESOS_SANDBOX/jdk*/&quot;,
          &quot;KAFKA_ADVERTISE_HOST&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_CREATE_TOPICS_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_AUTO_LEADER_REBALANCE_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_BACKGROUND_THREADS&quot; : &quot;10&quot;,
          &quot;KAFKA_COMPRESSION_TYPE&quot; : &quot;producer&quot;,
          &quot;KAFKA_CONNECTIONS_MAX_IDLE_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES&quot; : &quot;3&quot;,
          &quot;KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_DEFAULT_REPLICATION_FACTOR&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1&quot;,
          &quot;KAFKA_DELETE_TOPIC_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_DISK_PATH&quot; : &quot;kafka-broker-data&quot;,
          &quot;KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS&quot; : &quot;3000&quot;,
          &quot;KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_HEAP_OPTS&quot; : &quot;-Xms512M -Xmx512M&quot;,
          &quot;KAFKA_INTER_BROKER_PROTOCOL_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS&quot; : &quot;300&quot;,
          &quot;KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE&quot; : &quot;10&quot;,
          &quot;KAFKA_LOG_CLEANER_BACKOFF_MS&quot; : &quot;15000&quot;,
          &quot;KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE&quot; : &quot;134217728&quot;,
          &quot;KAFKA_LOG_CLEANER_DELETE_RETENTION_MS&quot; : &quot;86400000&quot;,
          &quot;KAFKA_LOG_CLEANER_ENABLE&quot; : &quot;true&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR&quot; : &quot;0.9&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_BUFFER_SIZE&quot; : &quot;524288&quot;,
          &quot;KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND&quot; : &quot;1.7976931348623157E308&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO&quot; : &quot;0.5&quot;,
          &quot;KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_CLEANER_THREADS&quot; : &quot;1&quot;,
          &quot;KAFKA_LOG_CLEANUP_POLICY&quot; : &quot;delete&quot;,
          &quot;KAFKA_LOG_FLUSH_INTERVAL_MESSAGES&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_LOG_INDEX_INTERVAL_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_LOG_INDEX_SIZE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_LOG_MESSAGE_FORMAT_VERSION&quot; : &quot;2.1&quot;,
          &quot;KAFKA_LOG_PREALLOCATE&quot; : &quot;false&quot;,
          &quot;KAFKA_LOG_RETENTION_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;300000&quot;,
          &quot;KAFKA_LOG_RETENTION_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_HOURS&quot; : &quot;168&quot;,
          &quot;KAFKA_LOG_ROLL_JITTER_HOURS&quot; : &quot;0&quot;,
          &quot;KAFKA_LOG_SEGMENT_BYTES&quot; : &quot;1073741824&quot;,
          &quot;KAFKA_LOG_SEGMENT_DELETE_DELAY_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_MAX_CONNECTIONS&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP&quot; : &quot;2147483647&quot;,
          &quot;KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES&quot; : &quot;&quot;,
          &quot;KAFKA_MESSAGE_MAX_BYTES&quot; : &quot;1000012&quot;,
          &quot;KAFKA_METRICS_NUM_SAMPLES&quot; : &quot;2&quot;,
          &quot;KAFKA_METRICS_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka08.StatsdMetricsReporter&quot;,
          &quot;KAFKA_METRICS_SAMPLE_WINDOW_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_MIN_INSYNC_REPLICAS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_IO_THREADS&quot; : &quot;8&quot;,
          &quot;KAFKA_NUM_NETWORK_THREADS&quot; : &quot;3&quot;,
          &quot;KAFKA_NUM_PARTITIONS&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR&quot; : &quot;1&quot;,
          &quot;KAFKA_NUM_REPLICA_FETCHERS&quot; : &quot;1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS&quot; : &quot;-1&quot;,
          &quot;KAFKA_OFFSETS_COMMIT_TIMEOUT_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_OFFSETS_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS&quot; : &quot;600000&quot;,
          &quot;KAFKA_OFFSETS_RETENTION_MINUTES&quot; : &quot;1440&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC&quot; : &quot;0&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_OFFSET_METADATA_MAX_BYTES&quot; : &quot;4096&quot;,
          &quot;KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS&quot; : &quot;1000&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUESTS&quot; : &quot;500&quot;,
          &quot;KAFKA_QUEUED_MAX_REQUEST_BYTES&quot; : &quot;-1&quot;,
          &quot;KAFKA_QUOTA_CONSUMER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_PRODUCER_DEFAULT&quot; : &quot;9223372036854775807&quot;,
          &quot;KAFKA_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_NUM&quot; : &quot;11&quot;,
          &quot;KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_BACKOFF_MS&quot; : &quot;1000&quot;,
          &quot;KAFKA_REPLICA_FETCH_MAX_BYTES&quot; : &quot;1048576&quot;,
          &quot;KAFKA_REPLICA_FETCH_MIN_BYTES&quot; : &quot;1&quot;,
          &quot;KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES&quot; : &quot;10485760&quot;,
          &quot;KAFKA_REPLICA_FETCH_WAIT_MAX_MS&quot; : &quot;500&quot;,
          &quot;KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS&quot; : &quot;5000&quot;,
          &quot;KAFKA_REPLICA_LAG_TIME_MAX_MS&quot; : &quot;10000&quot;,
          &quot;KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;65536&quot;,
          &quot;KAFKA_REPLICA_SOCKET_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_REQUEST_TIMEOUT_MS&quot; : &quot;30000&quot;,
          &quot;KAFKA_RESERVED_BROKER_MAX_ID&quot; : &quot;1000&quot;,
          &quot;KAFKA_SOCKET_RECEIVE_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SOCKET_REQUEST_MAX_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_SOCKET_SEND_BUFFER_BYTES&quot; : &quot;102400&quot;,
          &quot;KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED&quot; : &quot;true&quot;,
          &quot;KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS&quot; : &quot;604800000&quot;,
          &quot;KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;60000&quot;,
          &quot;KAFKA_TRANSACTION_MAX_TIMEOUT_MS&quot; : &quot;900000&quot;,
          &quot;KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS&quot; : &quot;3600000&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE&quot; : &quot;5242880&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_MIN_ISR&quot; : &quot;2&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS&quot; : &quot;50&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR&quot; : &quot;3&quot;,
          &quot;KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES&quot; : &quot;104857600&quot;,
          &quot;KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE&quot; : &quot;false&quot;,
          &quot;KAFKA_VERSION_PATH&quot; : &quot;kafka_1.23-4.5.6&quot;,
          &quot;KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS&quot; : &quot;6000&quot;,
          &quot;KAFKA_ZOOKEEPER_SYNC_TIME_MS&quot; : &quot;2000&quot;,
          &quot;METRIC_REPORTERS&quot; : &quot;com.airbnb.kafka.kafka09.StatsdMetricsReporter&quot;,
          &quot;SECURE_JMX_ENABLED&quot; : &quot;false&quot;
        }
      },
      &quot;task-labels&quot; : { },
      &quot;health-check-spec&quot; : null,
      &quot;readiness-check-spec&quot; : {
        &quot;command&quot; : &quot;# The broker has started when it logs a specific \&quot;started\&quot; log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \&quot;Checking for started log line in $kafka_server_log_files.\&quot;\ngrep -q \&quot;INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\&quot; $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \&quot;Found started log line.\&quot;\nelse\n  echo \&quot;started log line not found. Exiting.\&quot;\n  exit 1\nfi\necho \&quot;Required log line found. Broker is ready.\&quot;\nexit 0\n&quot;,
        &quot;delay&quot; : 0,
        &quot;interval&quot; : 60,
        &quot;timeout&quot; : 120
      },
      &quot;config-files&quot; : [ {
        &quot;name&quot; : &quot;server-properties&quot;,
        &quot;relative-path&quot; : &quot;kafka_1.23-4.5.6/config/server.properties&quot;,
        &quot;template-content&quot; : &quot;# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \&quot;License\&quot;); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \&quot;AS IS\&quot; BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \&quot;listeners\&quot; if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\&quot;.\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n&quot;
      } ],
      &quot;discovery-spec&quot; : null,
      &quot;kill-grace-period&quot; : 30,
      &quot;transport-encryption&quot; : [ ]
    } ],
    &quot;placement-rule&quot; : {
      &quot;@type&quot; : &quot;AndRule&quot;,
      &quot;rules&quot; : [ {
        &quot;@type&quot; : &quot;IsLocalRegionRule&quot;
      }, {
        &quot;@type&quot; : &quot;MaxPerHostnameRule&quot;,
        &quot;max&quot; : 1,
        &quot;task-filter&quot; : {
          &quot;@type&quot; : &quot;RegexMatcher&quot;,
          &quot;pattern&quot; : &quot;kafka-.*&quot;
        }
      } ]
    },
    &quot;volumes&quot; : [ ],
    &quot;pre-reserved-role&quot; : &quot;*&quot;,
    &quot;secrets&quot; : [ ],
    &quot;share-pid-namespace&quot; : false,
    &quot;host-volumes&quot; : [ ],
    &quot;seccomp-unconfined&quot; : false,
    &quot;seccomp-profile-name&quot; : null
  } ]
}
INFO  2019-09-05 09:55:45,120 [Test worker] DefaultConfigurationUpdater:updateConfiguration(147): Skipping config diff: There is no old config target to diff against
INFO  2019-09-05 09:55:45,122 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'null' is different from new configuration '63be63de-0da5-40db-b586-105b0d1f712e'. 
INFO  2019-09-05 09:55:45,122 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
INFO  2019-09-05 09:55:45,122 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 63be63de-0da5-40db-b586-105b0d1f712e: OK
INFO  2019-09-05 09:55:45,122 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
INFO  2019-09-05 09:55:45,123 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
WARN  2019-09-05 09:55:45,123 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
INFO  2019-09-05 09:55:45,123 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-0-broker=RUNNING}
INFO  2019-09-05 09:55:45,123 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,123 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,124 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
WARN  2019-09-05 09:55:45,124 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
INFO  2019-09-05 09:55:45,124 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-1-broker=RUNNING}
INFO  2019-09-05 09:55:45,124 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,124 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,125 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
WARN  2019-09-05 09:55:45,125 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
INFO  2019-09-05 09:55:45,125 [Test worker] DeploymentStep:&lt;init&gt;(73): Goal states: {kafka-2-broker=RUNNING}
INFO  2019-09-05 09:55:45,125 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,125 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
INFO  2019-09-05 09:55:45,125 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
INFO  2019-09-05 09:55:45,126 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
INFO  2019-09-05 09:55:45,126 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
    Step: kafka-0:[broker] (PENDING)
    Step: kafka-1:[broker] (PENDING)
    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:45,126 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
INFO  2019-09-05 09:55:45,127 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
INFO  2019-09-05 09:55:45,127 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
INFO  2019-09-05 09:55:45,128 [Test worker] TokenBucket:&lt;init&gt;(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
</pre>
</span>
</div>
</div>
<div id="footer">
<p>
<div>
<label class="hidden" id="label-for-line-wrapping-toggle" for="line-wrapping-toggle">Wrap lines
<input id="line-wrapping-toggle" type="checkbox" autocomplete="off"/>
</label>
</div>Generated by 
<a href="http://www.gradle.org">Gradle 4.10</a> at Sep 5, 2019 9:55:45 AM</p>
</div>
</div>
</body>
</html>
