…INFO  2019-09-05 09:55:43,591 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
	  kafka:
    count: 3
.    placement: '[["@zone", "GROUP_BY", "3"]]'

    uris:
K      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
!      - https://test-url/jre.tgz
`      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
-      - https://test-url/libmesos-bundle.tgz
V      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
T      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
4      - https://test-url/artifacts/setup-helper.zip
K      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
_      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
;      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
'            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
"          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
/          KAFKA_DISK_PATH: "kafka-broker-data"
/          KAFKA_HEAP_OPTS: "-Xms512M -Xmx512M"
,          JAVA_HOME: "$MESOS_SANDBOX/jdk*/"
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

9          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

^          # setup-helper determines the correct listeners and security.inter.broker.protocol.
H          # it relies on the task IP being stored in MESOS_CONTAINER_IP
C          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
N          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
8          export SETUP_HELPER_LISTENERS=`cat listeners`
b          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
<          export SETUP_HELPER_SUPER_USERS=`cat super.users`

%          ./bootstrap -resolve=false

Q          # NOTE: We add some custom statsd libraries for statsd metrics as well
H          # as a custom zookeeper library to support our own ZK running
Q          # kerberized. The custom zk library does not do DNS reverse resolution
!          # of the ZK hostnames.
          #
@          # Additionally, we include a custom principal builder
C          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
8          # Clean up any pre-existing zookeeper library
A          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
E          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
V          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
K          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
H               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
1            template: server.properties.mustache
<            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
f            # The broker has started when it logs a specific "started" log line. An example is below:
c            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
E            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

M            echo "Checking for started log line in $kafka_server_log_files."
}            grep -q "INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)" $kafka_server_log_files
#            if [ $? -eq 0 ] ; then
-              echo "Found started log line."
            else
:              echo "started log line not found. Exiting."
              exit 1
            fi
=            echo "Required log line found. Broker is ready."
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:

  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

ªINFO  2019-09-05 09:55:43,738 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@40633e95[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=<null>]
˜INFO  2019-09-05 09:55:43,789 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@zone, GROUP_BY, 3]' resulted in placement rule: 'RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}'
¬INFO  2019-09-05 09:55:43,799 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
ÆINFO  2019-09-05 09:55:44,093 [Test worker] SchedulerBuilder:getDefaultScheduler(510): Unable to retrieve last configuration. Assuming that no prior deployment has completed
vINFO  2019-09-05 09:55:44,099 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 12 validators...
zINFO  2019-09-05 09:55:44,103 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
*        "@type" : "RoundRobinByZoneRule",
        "zone-count" : 3,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
¶INFO  2019-09-05 09:55:44,112 [Test worker] DefaultConfigurationUpdater:updateConfiguration(147): Skipping config diff: There is no old config target to diff against
INFO  2019-09-05 09:55:44,120 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'null' is different from new configuration '531e4dea-74ea-4592-b428-183f90e325aa'. 
≤INFO  2019-09-05 09:55:44,121 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
öINFO  2019-09-05 09:55:44,121 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 531e4dea-74ea-4592-b428-183f90e325aa: OK
ÖINFO  2019-09-05 09:55:44,122 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
ÉINFO  2019-09-05 09:55:44,130 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,139 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:44,142 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:44,143 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,143 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,144 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,144 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:44,145 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:44,145 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,145 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,145 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,146 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:44,146 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:44,146 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,147 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:44,149 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
êINFO  2019-09-05 09:55:44,150 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
nINFO  2019-09-05 09:55:44,158 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,165 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
ÑINFO  2019-09-05 09:55:44,165 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
úINFO  2019-09-05 09:55:44,190 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
úINFO  2019-09-05 09:55:44,197 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
…INFO  2019-09-05 09:55:44,526 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
	  kafka:
    count: 3
+    placement: '[["@hostname", "UNIQUE"]]'

    uris:
K      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
!      - https://test-url/jre.tgz
`      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
-      - https://test-url/libmesos-bundle.tgz
V      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
T      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
4      - https://test-url/artifacts/setup-helper.zip
K      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
_      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
;      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
'            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
"          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
/          KAFKA_DISK_PATH: "kafka-broker-data"
/          KAFKA_HEAP_OPTS: "-Xms512M -Xmx512M"
,          JAVA_HOME: "$MESOS_SANDBOX/jdk*/"
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

9          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

^          # setup-helper determines the correct listeners and security.inter.broker.protocol.
H          # it relies on the task IP being stored in MESOS_CONTAINER_IP
C          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
N          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
8          export SETUP_HELPER_LISTENERS=`cat listeners`
b          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
<          export SETUP_HELPER_SUPER_USERS=`cat super.users`

%          ./bootstrap -resolve=false

Q          # NOTE: We add some custom statsd libraries for statsd metrics as well
H          # as a custom zookeeper library to support our own ZK running
Q          # kerberized. The custom zk library does not do DNS reverse resolution
!          # of the ZK hostnames.
          #
@          # Additionally, we include a custom principal builder
C          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
8          # Clean up any pre-existing zookeeper library
A          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
E          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
V          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
K          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
H               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
1            template: server.properties.mustache
<            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
f            # The broker has started when it logs a specific "started" log line. An example is below:
c            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
E            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

M            echo "Checking for started log line in $kafka_server_log_files."
}            grep -q "INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)" $kafka_server_log_files
#            if [ $? -eq 0 ] ; then
-              echo "Found started log line."
            else
:              echo "started log line not found. Exiting."
              exit 1
            fi
=            echo "Required log line found. Broker is ready."
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:

  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

∫INFO  2019-09-05 09:55:44,531 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@6891c3a[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=<null>]
„INFO  2019-09-05 09:55:44,534 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@hostname, UNIQUE]' resulted in placement rule: 'MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}'
¬INFO  2019-09-05 09:55:44,535 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
¡INFO  2019-09-05 09:55:44,563 [Test worker] ConfigStore:fetch(168): Fetching configuration with ID=531e4dea-74ea-4592-b428-183f90e325aa from Configurations/531e4dea-74ea-4592-b428-183f90e325aa
ÉINFO  2019-09-05 09:55:44,566 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,567 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:44,567 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:44,568 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,568 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,568 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,569 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:44,569 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:44,569 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,570 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,570 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,570 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:44,571 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:44,571 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,571 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:44,571 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
äINFO  2019-09-05 09:55:44,573 [Test worker] SchedulerBuilder:getDefaultScheduler(497): Previous deploy plan state: Plan: deploy (PENDING)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,574 [Test worker] SchedulerBuilder:getDefaultScheduler(503): Deployment has not previously completed
vINFO  2019-09-05 09:55:44,574 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 13 validators...
≠INFO  2019-09-05 09:55:44,574 [Test worker] DefaultConfigurationUpdater:updateConfiguration(123): Loading current target configuration: 531e4dea-74ea-4592-b428-183f90e325aa
zINFO  2019-09-05 09:55:44,576 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
(        "@type" : "MaxPerHostnameRule",
        "max" : 1,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
wINFO  2019-09-05 09:55:44,583 [Test worker] DefaultConfigurationUpdater:updateConfiguration(151): Prior target config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
*        "@type" : "RoundRobinByZoneRule",
        "zone-count" : 3,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
zINFO  2019-09-05 09:55:44,599 [Test worker] DefaultConfigurationUpdater:printConfigDiff(330): Difference between configs:
--- ServiceSpec.old
+++ ServiceSpec.new
@@ -233,6 +233,6 @@
'         "@type" : "IsLocalRegionRule"
       }, {
+-        "@type" : "RoundRobinByZoneRule",
-        "zone-count" : 3,
)+        "@type" : "MaxPerHostnameRule",
+        "max" : 1,
         "task-filter" : {
%           "@type" : "RegexMatcher",
ıWARN  2019-09-05 09:55:44,607 [Test worker] DefaultConfigurationUpdater:updateConfiguration(173): New configuration failed validation against current target configuration 531e4dea-74ea-4592-b428-183f90e325aa, with 1 errors across 13 validators:
1: Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' => 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false
≤INFO  2019-09-05 09:55:44,608 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
öINFO  2019-09-05 09:55:44,608 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 531e4dea-74ea-4592-b428-183f90e325aa: OK
ÖINFO  2019-09-05 09:55:44,608 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
ˇWARN  2019-09-05 09:55:44,608 [Test worker] SchedulerBuilder:getDefaultScheduler(522): Failed to update configuration due to validation errors: [Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' => 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false]
ÉINFO  2019-09-05 09:55:44,609 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,609 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:44,610 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:44,610 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,610 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,610 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,611 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:44,611 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:44,611 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,611 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,612 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,612 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:44,612 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:44,613 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,613 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:44,613 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
êINFO  2019-09-05 09:55:44,613 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
lINFO  2019-09-05 09:55:44,615 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (ERROR)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
Errors:
Ô  Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' => 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false
INFO  2019-09-05 09:55:44,615 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
ÑINFO  2019-09-05 09:55:44,616 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
úINFO  2019-09-05 09:55:44,617 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
úINFO  2019-09-05 09:55:44,618 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
qINFO  2019-09-05 09:55:44,622 [Test worker] ServiceTestRunner:run(383): SEND:   Framework registration completed
âINFO  2019-09-05 09:55:44,628 [Test worker] FrameworkScheduler:registered(163): Registered framework with frameworkId: test-framework-id
éINFO  2019-09-05 09:55:44,635 [Test worker] ExplicitReconciler:start(110): Added 0 unreconciled tasks to reconciler: 0 tasks to reconcile: []
qINFO  2019-09-05 09:55:44,635 [Test worker] ExplicitReconciler:reconcile(182): Completed explicit reconciliation
wINFO  2019-09-05 09:55:44,636 [Test worker] ImplicitReconciler:lambda$static$0(38): Triggering implicit reconciliation
mINFO  2019-09-05 09:55:44,637 [Test worker] ServiceTestRunner:run(376): EXPECT: Plan deploy has status ERROR
…INFO  2019-09-05 09:55:44,680 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
	  kafka:
    count: 3
0    placement: '[["hostname", "MAX_PER", "1"]]'

    uris:
K      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
!      - https://test-url/jre.tgz
`      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
-      - https://test-url/libmesos-bundle.tgz
V      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
T      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
4      - https://test-url/artifacts/setup-helper.zip
K      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
_      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
;      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
'            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
"          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
/          KAFKA_DISK_PATH: "kafka-broker-data"
/          KAFKA_HEAP_OPTS: "-Xms512M -Xmx512M"
,          JAVA_HOME: "$MESOS_SANDBOX/jdk*/"
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

9          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

^          # setup-helper determines the correct listeners and security.inter.broker.protocol.
H          # it relies on the task IP being stored in MESOS_CONTAINER_IP
C          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
N          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
8          export SETUP_HELPER_LISTENERS=`cat listeners`
b          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
<          export SETUP_HELPER_SUPER_USERS=`cat super.users`

%          ./bootstrap -resolve=false

Q          # NOTE: We add some custom statsd libraries for statsd metrics as well
H          # as a custom zookeeper library to support our own ZK running
Q          # kerberized. The custom zk library does not do DNS reverse resolution
!          # of the ZK hostnames.
          #
@          # Additionally, we include a custom principal builder
C          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
8          # Clean up any pre-existing zookeeper library
A          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
E          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
V          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
K          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
H               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
1            template: server.properties.mustache
<            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
f            # The broker has started when it logs a specific "started" log line. An example is below:
c            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
E            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

M            echo "Checking for started log line in $kafka_server_log_files."
}            grep -q "INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)" $kafka_server_log_files
#            if [ $? -eq 0 ] ; then
-              echo "Found started log line."
            else
:              echo "started log line not found. Exiting."
              exit 1
            fi
=            echo "Required log line found. Broker is ready."
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:

  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

ªINFO  2019-09-05 09:55:44,684 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@5902b63f[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=<null>]
ÊINFO  2019-09-05 09:55:44,686 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[hostname, MAX_PER, 1]' resulted in placement rule: 'MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}'
¬INFO  2019-09-05 09:55:44,686 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
ÆINFO  2019-09-05 09:55:44,709 [Test worker] SchedulerBuilder:getDefaultScheduler(510): Unable to retrieve last configuration. Assuming that no prior deployment has completed
vINFO  2019-09-05 09:55:44,709 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 12 validators...
zINFO  2019-09-05 09:55:44,710 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
(        "@type" : "MaxPerHostnameRule",
        "max" : 1,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
¶INFO  2019-09-05 09:55:44,712 [Test worker] DefaultConfigurationUpdater:updateConfiguration(147): Skipping config diff: There is no old config target to diff against
INFO  2019-09-05 09:55:44,714 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'null' is different from new configuration 'e3544347-4562-461b-a5e3-ae49b9027d3f'. 
≤INFO  2019-09-05 09:55:44,715 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
öINFO  2019-09-05 09:55:44,715 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - e3544347-4562-461b-a5e3-ae49b9027d3f: OK
ÖINFO  2019-09-05 09:55:44,715 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
ÉINFO  2019-09-05 09:55:44,715 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,716 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:44,716 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:44,716 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,716 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,717 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,717 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:44,717 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:44,717 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,718 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,718 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,718 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:44,718 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:44,719 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,719 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:44,719 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
êINFO  2019-09-05 09:55:44,719 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
nINFO  2019-09-05 09:55:44,720 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,720 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
ÑINFO  2019-09-05 09:55:44,721 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
úINFO  2019-09-05 09:55:44,722 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
úINFO  2019-09-05 09:55:44,723 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
…INFO  2019-09-05 09:55:44,762 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
	  kafka:
    count: 3
-    placement: '[["@zone", "MAX_PER", "3"]]'

    uris:
K      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
!      - https://test-url/jre.tgz
`      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
-      - https://test-url/libmesos-bundle.tgz
V      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
T      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
4      - https://test-url/artifacts/setup-helper.zip
K      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
_      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
;      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
'            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
"          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
/          KAFKA_DISK_PATH: "kafka-broker-data"
/          KAFKA_HEAP_OPTS: "-Xms512M -Xmx512M"
,          JAVA_HOME: "$MESOS_SANDBOX/jdk*/"
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

9          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

^          # setup-helper determines the correct listeners and security.inter.broker.protocol.
H          # it relies on the task IP being stored in MESOS_CONTAINER_IP
C          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
N          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
8          export SETUP_HELPER_LISTENERS=`cat listeners`
b          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
<          export SETUP_HELPER_SUPER_USERS=`cat super.users`

%          ./bootstrap -resolve=false

Q          # NOTE: We add some custom statsd libraries for statsd metrics as well
H          # as a custom zookeeper library to support our own ZK running
Q          # kerberized. The custom zk library does not do DNS reverse resolution
!          # of the ZK hostnames.
          #
@          # Additionally, we include a custom principal builder
C          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
8          # Clean up any pre-existing zookeeper library
A          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
E          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
V          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
K          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
H               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
1            template: server.properties.mustache
<            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
f            # The broker has started when it logs a specific "started" log line. An example is below:
c            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
E            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

M            echo "Checking for started log line in $kafka_server_log_files."
}            grep -q "INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)" $kafka_server_log_files
#            if [ $? -eq 0 ] ; then
-              echo "Found started log line."
            else
:              echo "started log line not found. Exiting."
              exit 1
            fi
=            echo "Required log line found. Broker is ready."
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:

  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

ªINFO  2019-09-05 09:55:44,766 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@2c884f5a[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=<null>]
ﬂINFO  2019-09-05 09:55:44,768 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@zone, MAX_PER, 3]' resulted in placement rule: 'com.mesosphere.sdk.offer.evaluate.placement.MaxPerZoneRule@2a79791b'
¬INFO  2019-09-05 09:55:44,769 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
ÆINFO  2019-09-05 09:55:44,792 [Test worker] SchedulerBuilder:getDefaultScheduler(510): Unable to retrieve last configuration. Assuming that no prior deployment has completed
vINFO  2019-09-05 09:55:44,793 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 12 validators...
zINFO  2019-09-05 09:55:44,795 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
$        "@type" : "MaxPerZoneRule",
        "max" : 3,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
¶INFO  2019-09-05 09:55:44,797 [Test worker] DefaultConfigurationUpdater:updateConfiguration(147): Skipping config diff: There is no old config target to diff against
INFO  2019-09-05 09:55:44,800 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'null' is different from new configuration 'bfc361b7-b0b3-4082-a9e8-9d5da93c801a'. 
≤INFO  2019-09-05 09:55:44,801 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
öINFO  2019-09-05 09:55:44,801 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - bfc361b7-b0b3-4082-a9e8-9d5da93c801a: OK
ÖINFO  2019-09-05 09:55:44,801 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
ÉINFO  2019-09-05 09:55:44,802 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,802 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:44,802 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:44,803 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,803 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,803 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,803 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:44,804 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:44,804 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,804 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,804 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,805 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:44,805 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:44,805 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,806 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:44,806 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
êINFO  2019-09-05 09:55:44,806 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
nINFO  2019-09-05 09:55:44,807 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,808 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
ÑINFO  2019-09-05 09:55:44,808 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
úINFO  2019-09-05 09:55:44,809 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
úINFO  2019-09-05 09:55:44,810 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
…INFO  2019-09-05 09:55:44,862 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
	  kafka:
    count: 3
.    placement: '[["@zone", "GROUP_BY", "3"]]'

    uris:
K      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
!      - https://test-url/jre.tgz
`      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
-      - https://test-url/libmesos-bundle.tgz
V      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
T      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
4      - https://test-url/artifacts/setup-helper.zip
K      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
_      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
;      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
'            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
"          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
/          KAFKA_DISK_PATH: "kafka-broker-data"
/          KAFKA_HEAP_OPTS: "-Xms512M -Xmx512M"
,          JAVA_HOME: "$MESOS_SANDBOX/jdk*/"
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

9          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

^          # setup-helper determines the correct listeners and security.inter.broker.protocol.
H          # it relies on the task IP being stored in MESOS_CONTAINER_IP
C          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
N          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
8          export SETUP_HELPER_LISTENERS=`cat listeners`
b          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
<          export SETUP_HELPER_SUPER_USERS=`cat super.users`

%          ./bootstrap -resolve=false

Q          # NOTE: We add some custom statsd libraries for statsd metrics as well
H          # as a custom zookeeper library to support our own ZK running
Q          # kerberized. The custom zk library does not do DNS reverse resolution
!          # of the ZK hostnames.
          #
@          # Additionally, we include a custom principal builder
C          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
8          # Clean up any pre-existing zookeeper library
A          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
E          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
V          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
K          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
H               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
1            template: server.properties.mustache
<            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
f            # The broker has started when it logs a specific "started" log line. An example is below:
c            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
E            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

M            echo "Checking for started log line in $kafka_server_log_files."
}            grep -q "INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)" $kafka_server_log_files
#            if [ $? -eq 0 ] ; then
-              echo "Found started log line."
            else
:              echo "started log line not found. Exiting."
              exit 1
            fi
=            echo "Required log line found. Broker is ready."
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:

  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

∫INFO  2019-09-05 09:55:44,866 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@1bccf2f[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=<null>]
˜INFO  2019-09-05 09:55:44,868 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@zone, GROUP_BY, 3]' resulted in placement rule: 'RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}'
¬INFO  2019-09-05 09:55:44,869 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
¡INFO  2019-09-05 09:55:44,890 [Test worker] ConfigStore:fetch(168): Fetching configuration with ID=bfc361b7-b0b3-4082-a9e8-9d5da93c801a from Configurations/bfc361b7-b0b3-4082-a9e8-9d5da93c801a
ÉINFO  2019-09-05 09:55:44,893 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,894 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:44,894 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:44,894 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,894 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,895 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,895 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:44,895 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:44,896 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,896 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,896 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,897 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:44,897 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:44,897 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,897 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:44,898 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
äINFO  2019-09-05 09:55:44,898 [Test worker] SchedulerBuilder:getDefaultScheduler(497): Previous deploy plan state: Plan: deploy (PENDING)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,899 [Test worker] SchedulerBuilder:getDefaultScheduler(503): Deployment has not previously completed
vINFO  2019-09-05 09:55:44,899 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 13 validators...
≠INFO  2019-09-05 09:55:44,900 [Test worker] DefaultConfigurationUpdater:updateConfiguration(123): Loading current target configuration: bfc361b7-b0b3-4082-a9e8-9d5da93c801a
zINFO  2019-09-05 09:55:44,902 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
*        "@type" : "RoundRobinByZoneRule",
        "zone-count" : 3,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
wINFO  2019-09-05 09:55:44,916 [Test worker] DefaultConfigurationUpdater:updateConfiguration(151): Prior target config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
$        "@type" : "MaxPerZoneRule",
        "max" : 3,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
zINFO  2019-09-05 09:55:44,920 [Test worker] DefaultConfigurationUpdater:printConfigDiff(330): Difference between configs:
--- ServiceSpec.old
+++ ServiceSpec.new
@@ -233,6 +233,6 @@
'         "@type" : "IsLocalRegionRule"
       }, {
%-        "@type" : "MaxPerZoneRule",
-        "max" : 3,
++        "@type" : "RoundRobinByZoneRule",
+        "zone-count" : 3,
         "task-filter" : {
%           "@type" : "RegexMatcher",
êINFO  2019-09-05 09:55:44,923 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'bfc361b7-b0b3-4082-a9e8-9d5da93c801a' is different from new configuration '9167fa5b-28b9-46b0-bfa7-305c6f3d696b'. 
≤INFO  2019-09-05 09:55:44,924 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 2 listed configurations before cleanup:
öINFO  2019-09-05 09:55:44,924 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 9167fa5b-28b9-46b0-bfa7-305c6f3d696b: OK
öINFO  2019-09-05 09:55:44,924 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - bfc361b7-b0b3-4082-a9e8-9d5da93c801a: OK
©INFO  2019-09-05 09:55:44,925 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 1 unused configs: [bfc361b7-b0b3-4082-a9e8-9d5da93c801a]
ÉINFO  2019-09-05 09:55:44,925 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,925 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:44,926 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:44,926 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,926 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,927 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,927 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:44,927 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:44,927 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,928 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:44,928 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:44,928 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:44,929 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:44,929 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:44,929 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:44,929 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
êINFO  2019-09-05 09:55:44,929 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
nINFO  2019-09-05 09:55:44,930 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:44,931 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
ÑINFO  2019-09-05 09:55:44,931 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
úINFO  2019-09-05 09:55:44,932 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
úINFO  2019-09-05 09:55:44,933 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
qINFO  2019-09-05 09:55:44,936 [Test worker] ServiceTestRunner:run(383): SEND:   Framework registration completed
âINFO  2019-09-05 09:55:44,936 [Test worker] FrameworkScheduler:registered(163): Registered framework with frameworkId: test-framework-id
éINFO  2019-09-05 09:55:44,937 [Test worker] ExplicitReconciler:start(110): Added 0 unreconciled tasks to reconciler: 0 tasks to reconcile: []
qINFO  2019-09-05 09:55:44,937 [Test worker] ExplicitReconciler:reconcile(182): Completed explicit reconciliation
wINFO  2019-09-05 09:55:44,937 [Test worker] ImplicitReconciler:lambda$static$0(38): Triggering implicit reconciliation
{INFO  2019-09-05 09:55:44,937 [Test worker] ServiceTestRunner:run(376): EXPECT: Deploy plan does not have ERROR as status.
…INFO  2019-09-05 09:55:44,977 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
	  kafka:
    count: 3
+    placement: '[["@hostname", "UNIQUE"]]'

    uris:
K      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
!      - https://test-url/jre.tgz
`      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
-      - https://test-url/libmesos-bundle.tgz
V      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
T      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
4      - https://test-url/artifacts/setup-helper.zip
K      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
_      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
;      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
'            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
"          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
/          KAFKA_DISK_PATH: "kafka-broker-data"
/          KAFKA_HEAP_OPTS: "-Xms512M -Xmx512M"
,          JAVA_HOME: "$MESOS_SANDBOX/jdk*/"
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

9          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

^          # setup-helper determines the correct listeners and security.inter.broker.protocol.
H          # it relies on the task IP being stored in MESOS_CONTAINER_IP
C          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
N          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
8          export SETUP_HELPER_LISTENERS=`cat listeners`
b          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
<          export SETUP_HELPER_SUPER_USERS=`cat super.users`

%          ./bootstrap -resolve=false

Q          # NOTE: We add some custom statsd libraries for statsd metrics as well
H          # as a custom zookeeper library to support our own ZK running
Q          # kerberized. The custom zk library does not do DNS reverse resolution
!          # of the ZK hostnames.
          #
@          # Additionally, we include a custom principal builder
C          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
8          # Clean up any pre-existing zookeeper library
A          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
E          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
V          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
K          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
H               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
1            template: server.properties.mustache
<            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
f            # The broker has started when it logs a specific "started" log line. An example is below:
c            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
E            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

M            echo "Checking for started log line in $kafka_server_log_files."
}            grep -q "INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)" $kafka_server_log_files
#            if [ $? -eq 0 ] ; then
-              echo "Found started log line."
            else
:              echo "started log line not found. Exiting."
              exit 1
            fi
=            echo "Required log line found. Broker is ready."
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:

  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

ªINFO  2019-09-05 09:55:44,980 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@547766fd[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=<null>]
„INFO  2019-09-05 09:55:44,982 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@hostname, UNIQUE]' resulted in placement rule: 'MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}'
¬INFO  2019-09-05 09:55:44,983 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
ÆINFO  2019-09-05 09:55:45,000 [Test worker] SchedulerBuilder:getDefaultScheduler(510): Unable to retrieve last configuration. Assuming that no prior deployment has completed
vINFO  2019-09-05 09:55:45,000 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 12 validators...
zINFO  2019-09-05 09:55:45,001 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
(        "@type" : "MaxPerHostnameRule",
        "max" : 1,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
¶INFO  2019-09-05 09:55:45,002 [Test worker] DefaultConfigurationUpdater:updateConfiguration(147): Skipping config diff: There is no old config target to diff against
INFO  2019-09-05 09:55:45,005 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'null' is different from new configuration '988703c5-d9dc-4002-9100-78476fd71a2a'. 
≤INFO  2019-09-05 09:55:45,005 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
öINFO  2019-09-05 09:55:45,005 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 988703c5-d9dc-4002-9100-78476fd71a2a: OK
ÖINFO  2019-09-05 09:55:45,005 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
ÉINFO  2019-09-05 09:55:45,006 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,006 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:45,006 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:45,006 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,007 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:45,007 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,007 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:45,008 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:45,008 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,008 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:45,008 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,009 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:45,009 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:45,009 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,009 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:45,010 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
êINFO  2019-09-05 09:55:45,010 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
nINFO  2019-09-05 09:55:45,010 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:45,011 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
ÑINFO  2019-09-05 09:55:45,011 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
úINFO  2019-09-05 09:55:45,012 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
úINFO  2019-09-05 09:55:45,013 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
…INFO  2019-09-05 09:55:45,032 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
	  kafka:
    count: 3
.    placement: '[["@zone", "GROUP_BY", "3"]]'

    uris:
K      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
!      - https://test-url/jre.tgz
`      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
-      - https://test-url/libmesos-bundle.tgz
V      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
T      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
4      - https://test-url/artifacts/setup-helper.zip
K      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
_      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
;      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
'            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
"          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
/          KAFKA_DISK_PATH: "kafka-broker-data"
/          KAFKA_HEAP_OPTS: "-Xms512M -Xmx512M"
,          JAVA_HOME: "$MESOS_SANDBOX/jdk*/"
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

9          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

^          # setup-helper determines the correct listeners and security.inter.broker.protocol.
H          # it relies on the task IP being stored in MESOS_CONTAINER_IP
C          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
N          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
8          export SETUP_HELPER_LISTENERS=`cat listeners`
b          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
<          export SETUP_HELPER_SUPER_USERS=`cat super.users`

%          ./bootstrap -resolve=false

Q          # NOTE: We add some custom statsd libraries for statsd metrics as well
H          # as a custom zookeeper library to support our own ZK running
Q          # kerberized. The custom zk library does not do DNS reverse resolution
!          # of the ZK hostnames.
          #
@          # Additionally, we include a custom principal builder
C          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
8          # Clean up any pre-existing zookeeper library
A          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
E          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
V          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
K          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
H               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
1            template: server.properties.mustache
<            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
f            # The broker has started when it logs a specific "started" log line. An example is below:
c            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
E            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

M            echo "Checking for started log line in $kafka_server_log_files."
}            grep -q "INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)" $kafka_server_log_files
#            if [ $? -eq 0 ] ; then
-              echo "Found started log line."
            else
:              echo "started log line not found. Exiting."
              exit 1
            fi
=            echo "Required log line found. Broker is ready."
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:

  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

ªINFO  2019-09-05 09:55:45,035 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@22a6b4e3[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=<null>]
˜INFO  2019-09-05 09:55:45,036 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[@zone, GROUP_BY, 3]' resulted in placement rule: 'RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}'
¬INFO  2019-09-05 09:55:45,037 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
¡INFO  2019-09-05 09:55:45,053 [Test worker] ConfigStore:fetch(168): Fetching configuration with ID=988703c5-d9dc-4002-9100-78476fd71a2a from Configurations/988703c5-d9dc-4002-9100-78476fd71a2a
ÉINFO  2019-09-05 09:55:45,055 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,055 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:45,056 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:45,056 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,056 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:45,056 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,057 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:45,057 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:45,057 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,057 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:45,058 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,058 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:45,058 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:45,058 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,059 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:45,059 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
äINFO  2019-09-05 09:55:45,059 [Test worker] SchedulerBuilder:getDefaultScheduler(497): Previous deploy plan state: Plan: deploy (PENDING)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:45,060 [Test worker] SchedulerBuilder:getDefaultScheduler(503): Deployment has not previously completed
vINFO  2019-09-05 09:55:45,060 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 13 validators...
≠INFO  2019-09-05 09:55:45,060 [Test worker] DefaultConfigurationUpdater:updateConfiguration(123): Loading current target configuration: 988703c5-d9dc-4002-9100-78476fd71a2a
zINFO  2019-09-05 09:55:45,061 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
*        "@type" : "RoundRobinByZoneRule",
        "zone-count" : 3,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
wINFO  2019-09-05 09:55:45,063 [Test worker] DefaultConfigurationUpdater:updateConfiguration(151): Prior target config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
(        "@type" : "MaxPerHostnameRule",
        "max" : 1,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
zINFO  2019-09-05 09:55:45,065 [Test worker] DefaultConfigurationUpdater:printConfigDiff(330): Difference between configs:
--- ServiceSpec.old
+++ ServiceSpec.new
@@ -233,6 +233,6 @@
'         "@type" : "IsLocalRegionRule"
       }, {
)-        "@type" : "MaxPerHostnameRule",
-        "max" : 1,
++        "@type" : "RoundRobinByZoneRule",
+        "zone-count" : 3,
         "task-filter" : {
%           "@type" : "RegexMatcher",
ıWARN  2019-09-05 09:55:45,067 [Test worker] DefaultConfigurationUpdater:updateConfiguration(173): New configuration failed validation against current target configuration 988703c5-d9dc-4002-9100-78476fd71a2a, with 1 errors across 13 validators:
1: Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' => 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false
≤INFO  2019-09-05 09:55:45,067 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
öINFO  2019-09-05 09:55:45,068 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 988703c5-d9dc-4002-9100-78476fd71a2a: OK
ÖINFO  2019-09-05 09:55:45,068 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
ˇWARN  2019-09-05 09:55:45,068 [Test worker] SchedulerBuilder:getDefaultScheduler(522): Failed to update configuration due to validation errors: [Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' => 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false]
ÉINFO  2019-09-05 09:55:45,068 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,069 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:45,069 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:45,069 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,070 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:45,070 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,070 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:45,071 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:45,071 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,071 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:45,071 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,072 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:45,072 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:45,072 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,073 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:45,073 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
êINFO  2019-09-05 09:55:45,073 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
lINFO  2019-09-05 09:55:45,074 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (ERROR)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
Errors:
Ô  Field: 'kafka.PlacementRule'; Transition: 'Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}]' => 'Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Message: 'PlacementRule cannot change from Optional[AndRule{rules=[IsLocalRegionRule, MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}]}] to Optional[AndRule{rules=[IsLocalRegionRule, RoundRobinByZoneRule{zone-count=Optional[3], task-filter=RegexMatcher{pattern='kafka-.*'}}]}]'; Fatal: false
INFO  2019-09-05 09:55:45,074 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
ÑINFO  2019-09-05 09:55:45,075 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
úINFO  2019-09-05 09:55:45,076 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
úINFO  2019-09-05 09:55:45,076 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
qINFO  2019-09-05 09:55:45,079 [Test worker] ServiceTestRunner:run(383): SEND:   Framework registration completed
âINFO  2019-09-05 09:55:45,079 [Test worker] FrameworkScheduler:registered(163): Registered framework with frameworkId: test-framework-id
éINFO  2019-09-05 09:55:45,080 [Test worker] ExplicitReconciler:start(110): Added 0 unreconciled tasks to reconciler: 0 tasks to reconcile: []
qINFO  2019-09-05 09:55:45,081 [Test worker] ExplicitReconciler:reconcile(182): Completed explicit reconciliation
wINFO  2019-09-05 09:55:45,081 [Test worker] ImplicitReconciler:lambda$static$0(38): Triggering implicit reconciliation
mINFO  2019-09-05 09:55:45,081 [Test worker] ServiceTestRunner:run(376): EXPECT: Plan deploy has status ERROR
…INFO  2019-09-05 09:55:45,099 [Test worker] RawServiceSpec:build(138): Rendered ServiceSpec from /Users/michaelbi/dev/mesosphere/sdk-operator/dcos-kafka-service/frameworks/kafka/src/main/dist/svc.yml:
Missing template values: []
name: kafka
scheduler:
  principal: 
  user: nobody
pods:
	  kafka:
    count: 3
0    placement: '[["hostname", "MAX_PER", "1"]]'

    uris:
K      - https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz
!      - https://test-url/jre.tgz
`      - https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip
-      - https://test-url/libmesos-bundle.tgz
V      - https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar
T      - https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar
4      - https://test-url/artifacts/setup-helper.zip
K      - https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar
_      - https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar
;      - https://downloads.mesosphere.com/kafka/assets/nc64
    rlimits:
      RLIMIT_NOFILE:
        soft: 128000
        hard: 128000
    tasks:
      broker:
        cpus: 1.0
        memory: 2048
        ports:
          broker:
            port: 0
'            env-key: KAFKA_BROKER_PORT
            advertise: true
            vip:
              prefix: broker
              port: 9092
        volume:
"          path: kafka-broker-data
          type: ROOT
          size: 5000
        env:
/          KAFKA_DISK_PATH: "kafka-broker-data"
/          KAFKA_HEAP_OPTS: "-Xms512M -Xmx512M"
,          JAVA_HOME: "$MESOS_SANDBOX/jdk*/"
        goal: RUNNING
        cmd: |
          # Exit on any error.
          set -e

9          export JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)

^          # setup-helper determines the correct listeners and security.inter.broker.protocol.
H          # it relies on the task IP being stored in MESOS_CONTAINER_IP
C          export MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )
          ./setup-helper
N          export SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`
8          export SETUP_HELPER_LISTENERS=`cat listeners`
b          export SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`
<          export SETUP_HELPER_SUPER_USERS=`cat super.users`

%          ./bootstrap -resolve=false

Q          # NOTE: We add some custom statsd libraries for statsd metrics as well
H          # as a custom zookeeper library to support our own ZK running
Q          # kerberized. The custom zk library does not do DNS reverse resolution
!          # of the ZK hostnames.
          #
@          # Additionally, we include a custom principal builder
C          mv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
8          # Clean up any pre-existing zookeeper library
A          rm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar
E          mv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
V          mv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/
          # Start kafka.
K          exec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \
H               $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties
        configs:
          server-properties:
1            template: server.properties.mustache
<            dest: kafka_1.23-4.5.6/config/server.properties
        readiness-check:
          cmd: |
f            # The broker has started when it logs a specific "started" log line. An example is below:
c            # [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
E            kafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*

M            echo "Checking for started log line in $kafka_server_log_files."
}            grep -q "INFO \[KafkaServer id=$POD_INSTANCE_INDEX\] started (kafka.server.KafkaServer)" $kafka_server_log_files
#            if [ $? -eq 0 ] ; then
-              echo "Found started log line."
            else
:              echo "started log line not found. Exiting."
              exit 1
            fi
=            echo "Required log line found. Broker is ready."
            exit 0
          interval: 60
          delay: 0
          timeout: 120
        kill-grace-period: 30
plans:

  deploy:
    strategy: serial
    phases:
      broker:
        strategy: serial
        pod: kafka

ªINFO  2019-09-05 09:55:45,101 [Test worker] YAMLToInternalMappers:convertServiceSpec(146): Using framework config : com.mesosphere.sdk.framework.FrameworkConfig@29f8d30f[frameworkName=kafka,principal=kafka-principal,user=nobody,zookeeperHostPort=master.mesos:2181,preReservedRoles=[],role=kafka-role,webUrl=<null>]
ÊINFO  2019-09-05 09:55:45,102 [Test worker] MarathonConstraintParser:parseRow(118): Marathon-style row '[hostname, MAX_PER, 1]' resulted in placement rule: 'MaxPerHostnameRule{max=1, task-filter=RegexMatcher{pattern='kafka-.*'}}'
¬INFO  2019-09-05 09:55:45,103 [Test worker] SchedulerBuilder:build(360): Updating pods with local region placement rule: region awareness=false, scheduler region=Optional[test-scheduler-region]
ÆINFO  2019-09-05 09:55:45,118 [Test worker] SchedulerBuilder:getDefaultScheduler(510): Unable to retrieve last configuration. Assuming that no prior deployment has completed
vINFO  2019-09-05 09:55:45,118 [Test worker] SchedulerBuilder:updateConfig(746): Updating config with 12 validators...
zINFO  2019-09-05 09:55:45,119 [Test worker] DefaultConfigurationUpdater:updateConfiguration(135): New prospective config:
{
  "name" : "kafka",
  "role" : "kafka-role",
#  "principal" : "kafka-principal",
  "user" : "nobody",
  "goal" : "RUNNING",
&  "region" : "test-scheduler-region",
  "web-url" : null,
%  "zookeeper" : "master.mesos:2181",
'  "replacement-failure-policy" : null,
  "pod-specs" : [ {
    "type" : "kafka",
    "user" : "nobody",
    "count" : 3,
"    "allow-decommission" : false,
    "image" : null,
    "networks" : [ ],
    "rlimits" : [ {
       "name" : "RLIMIT_NOFILE",
      "soft" : 128000,
      "hard" : 128000
	    } ],
õ    "uris" : [ "https://downloads.mesosphere.com/kafka/assets/kafka_1.23-4.5.6.tgz", "https://test-url/jre.tgz", "https://downloads.mesosphere.com/dcos-commons/artifacts/99.99.99-SNAPSHOT/bootstrap.zip", "https://test-url/libmesos-bundle.tgz", "https://downloads.mesosphere.com/kafka/assets/kafka-statsd-metrics2-0.5.3.jar", "https://downloads.mesosphere.com/kafka/assets/java-dogstatsd-client-2.3.jar", "https://test-url/artifacts/setup-helper.zip", "https://downloads.mesosphere.com/kafka/assets/zookeeper-3.4.13.jar", "https://downloads.mesosphere.com/kafka/assets/kafka-custom-principal-builder-1.0.0.jar", "https://downloads.mesosphere.com/kafka/assets/nc64" ],
    "task-specs" : [ {
      "name" : "broker",
      "goal" : "RUNNING",
      "essential" : true,
      "resource-set" : {
&        "id" : "broker-resource-set",
(        "resource-specifications" : [ {
+          "@type" : "DefaultResourceSpec",
          "name" : "cpus",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 1.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
+          "@type" : "DefaultResourceSpec",
          "name" : "mem",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 2048.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        }, {
$          "@type" : "NamedVIPSpec",
          "value" : {
            "type" : "RANGES",
            "ranges" : {
              "range" : [ {
                "begin" : 0,
                "end" : 0
              } ]
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
+          "principal" : "kafka-principal",
+          "env-key" : "KAFKA_BROKER_PORT",
"          "port-name" : "broker",
%          "visibility" : "EXTERNAL",
!          "network-names" : [ ],
          "protocol" : "tcp",
!          "vip-name" : "broker",
          "vip-port" : 9092,
          "name" : "ports",
          "ranges" : [ ]
        } ],
&        "volume-specifications" : [ {
)          "@type" : "DefaultVolumeSpec",
          "type" : "ROOT",
2          "container-path" : "kafka-broker-data",
          "profiles" : [ ],
          "name" : "disk",
          "value" : {
            "type" : "SCALAR",
            "scalar" : {
              "value" : 5000.0
            }
          },
!          "role" : "kafka-role",
%          "pre-reserved-role" : "*",
*          "principal" : "kafka-principal"
        } ],
        "role" : "kafka-role",
(        "principal" : "kafka-principal"
	      },
      "command-spec" : {
√
        "value" : "# Exit on any error.\nset -e\n\nexport JAVA_HOME=$(ls -d $MESOS_SANDBOX/jdk*/)\n\n# setup-helper determines the correct listeners and security.inter.broker.protocol.\n# it relies on the task IP being stored in MESOS_CONTAINER_IP\nexport MESOS_CONTAINER_IP=$( ./bootstrap --get-task-ip )\n./setup-helper\nexport SETUP_HELPER_ADVERTISED_LISTENERS=`cat advertised.listeners`\nexport SETUP_HELPER_LISTENERS=`cat listeners`\nexport SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL=`cat security.inter.broker.protocol`\nexport SETUP_HELPER_SUPER_USERS=`cat super.users`\n\n./bootstrap -resolve=false\n\n# NOTE: We add some custom statsd libraries for statsd metrics as well\n# as a custom zookeeper library to support our own ZK running\n# kerberized. The custom zk library does not do DNS reverse resolution\n# of the ZK hostnames.\n#\n# Additionally, we include a custom principal builder\nmv -v *statsd*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Clean up any pre-existing zookeeper library\nrm $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/zookeeper*.jar\nmv -v zookeeper*.jar $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\nmv -v kafka-custom-principal-builder* $MESOS_SANDBOX/kafka_1.23-4.5.6/libs/\n# Start kafka.\nexec $MESOS_SANDBOX/kafka_1.23-4.5.6/bin/kafka-server-start.sh \\\n     $MESOS_SANDBOX/kafka_1.23-4.5.6/config/server.properties\n",
        "environment" : {
0          "JAVA_HOME" : "$MESOS_SANDBOX/jdk*/",
+          "KAFKA_ADVERTISE_HOST" : "true",
6          "KAFKA_AUTO_CREATE_TOPICS_ENABLE" : "true",
9          "KAFKA_AUTO_LEADER_REBALANCE_ENABLE" : "true",
-          "KAFKA_BACKGROUND_THREADS" : "10",
1          "KAFKA_COMPRESSION_TYPE" : "producer",
6          "KAFKA_CONNECTIONS_MAX_IDLE_MS" : "600000",
7          "KAFKA_CONTROLLED_SHUTDOWN_ENABLE" : "true",
9          "KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES" : "3",
A          "KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS" : "5000",
:          "KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS" : "30000",
4          "KAFKA_DEFAULT_REPLICATION_FACTOR" : "1",
J          "KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1",
1          "KAFKA_DELETE_TOPIC_ENABLE" : "false",
3          "KAFKA_DISK_PATH" : "kafka-broker-data",
D          "KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
=          "KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS" : "3000",
;          "KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS" : "300000",
9          "KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_HEAP_OPTS" : "-Xms512M -Xmx512M",
9          "KAFKA_INTER_BROKER_PROTOCOL_VERSION" : "2.1",
C          "KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS" : "300",
A          "KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE" : "10",
4          "KAFKA_LOG_CLEANER_BACKOFF_MS" : "15000",
@          "KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE" : "134217728",
@          "KAFKA_LOG_CLEANER_DELETE_RETENTION_MS" : "86400000",
/          "KAFKA_LOG_CLEANER_ENABLE" : "true",
=          "KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR" : "0.9",
9          "KAFKA_LOG_CLEANER_IO_BUFFER_SIZE" : "524288",
R          "KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND" : "1.7976931348623157E308",
;          "KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO" : "0.5",
;          "KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS" : "0",
-          "KAFKA_LOG_CLEANER_THREADS" : "1",
1          "KAFKA_LOG_CLEANUP_POLICY" : "delete",
G          "KAFKA_LOG_FLUSH_INTERVAL_MESSAGES" : "9223372036854775807",
E          "KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
K          "KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS" : "9223372036854775807",
K          "KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS" : "60000",
5          "KAFKA_LOG_INDEX_INTERVAL_BYTES" : "4096",
9          "KAFKA_LOG_INDEX_SIZE_MAX_BYTES" : "10485760",
6          "KAFKA_LOG_MESSAGE_FORMAT_VERSION" : "2.1",
-          "KAFKA_LOG_PREALLOCATE" : "false",
.          "KAFKA_LOG_RETENTION_BYTES" : "-1",
>          "KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS" : "300000",
/          "KAFKA_LOG_RETENTION_HOURS" : "168",
*          "KAFKA_LOG_ROLL_HOURS" : "168",
/          "KAFKA_LOG_ROLL_JITTER_HOURS" : "0",
4          "KAFKA_LOG_SEGMENT_BYTES" : "1073741824",
9          "KAFKA_LOG_SEGMENT_DELETE_DELAY_MS" : "60000",
2          "KAFKA_MAX_CONNECTIONS" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP" : "2147483647",
9          "KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES" : "",
1          "KAFKA_MESSAGE_MAX_BYTES" : "1000012",
-          "KAFKA_METRICS_NUM_SAMPLES" : "2",
X          "KAFKA_METRICS_REPORTERS" : "com.airbnb.kafka.kafka08.StatsdMetricsReporter",
6          "KAFKA_METRICS_SAMPLE_WINDOW_MS" : "30000",
-          "KAFKA_MIN_INSYNC_REPLICAS" : "1",
(          "KAFKA_NUM_IO_THREADS" : "8",
-          "KAFKA_NUM_NETWORK_THREADS" : "3",
(          "KAFKA_NUM_PARTITIONS" : "1",
;          "KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR" : "1",
.          "KAFKA_NUM_REPLICA_FETCHERS" : "1",
7          "KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS" : "-1",
6          "KAFKA_OFFSETS_COMMIT_TIMEOUT_MS" : "5000",
8          "KAFKA_OFFSETS_LOAD_BUFFER_SIZE" : "5242880",
B          "KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS" : "600000",
6          "KAFKA_OFFSETS_RETENTION_MINUTES" : "1440",
9          "KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC" : "0",
7          "KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS" : "50",
:          "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR" : "3",
=          "KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES" : "104857600",
6          "KAFKA_OFFSET_METADATA_MAX_BYTES" : "4096",
G          "KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS" : "1000",
/          "KAFKA_QUEUED_MAX_REQUESTS" : "500",
3          "KAFKA_QUEUED_MAX_REQUEST_BYTES" : "-1",
B          "KAFKA_QUOTA_CONSUMER_DEFAULT" : "9223372036854775807",
B          "KAFKA_QUOTA_PRODUCER_DEFAULT" : "9223372036854775807",
+          "KAFKA_QUOTA_WINDOW_NUM" : "11",
3          "KAFKA_QUOTA_WINDOW_SIZE_SECONDS" : "1",
7          "KAFKA_REPLICATION_QUOTA_WINDOW_NUM" : "11",
?          "KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS" : "1",
5          "KAFKA_REPLICA_FETCH_BACKOFF_MS" : "1000",
7          "KAFKA_REPLICA_FETCH_MAX_BYTES" : "1048576",
1          "KAFKA_REPLICA_FETCH_MIN_BYTES" : "1",
A          "KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES" : "10485760",
5          "KAFKA_REPLICA_FETCH_WAIT_MAX_MS" : "500",
J          "KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS" : "5000",
5          "KAFKA_REPLICA_LAG_TIME_MAX_MS" : "10000",
A          "KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES" : "65536",
7          "KAFKA_REPLICA_SOCKET_TIMEOUT_MS" : "30000",
0          "KAFKA_REQUEST_TIMEOUT_MS" : "30000",
3          "KAFKA_RESERVED_BROKER_MAX_ID" : "1000",
:          "KAFKA_SOCKET_RECEIVE_BUFFER_BYTES" : "102400",
:          "KAFKA_SOCKET_REQUEST_MAX_BYTES" : "104857600",
7          "KAFKA_SOCKET_SEND_BUFFER_BYTES" : "102400",
@          "KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED" : "true",
@          "KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS" : "604800000",
Y          "KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS" : "60000",
9          "KAFKA_TRANSACTION_MAX_TIMEOUT_MS" : "900000",
Z          "KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS" : "3600000",
F          "KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE" : "5242880",
7          "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR" : "2",
?          "KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS" : "50",
B          "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR" : "3",
E          "KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES" : "104857600",
<          "KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE" : "false",
5          "KAFKA_VERSION_PATH" : "kafka_1.23-4.5.6",
9          "KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS" : "6000",
3          "KAFKA_ZOOKEEPER_SYNC_TIME_MS" : "2000",
Q          "METRIC_REPORTERS" : "com.airbnb.kafka.kafka09.StatsdMetricsReporter",
)          "SECURE_JMX_ENABLED" : "false"

        }
	      },
      "task-labels" : { },
"      "health-check-spec" : null,
!      "readiness-check-spec" : {
Ä        "command" : "# The broker has started when it logs a specific \"started\" log line. An example is below:\n# [2017-06-14 22:20:55,464] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)\nkafka_server_log_files=kafka_1.23-4.5.6/logs/server.log*\n\necho \"Checking for started log line in $kafka_server_log_files.\"\ngrep -q \"INFO \\[KafkaServer id=$POD_INSTANCE_INDEX\\] started (kafka.server.KafkaServer)\" $kafka_server_log_files\nif [ $? -eq 0 ] ; then\n  echo \"Found started log line.\"\nelse\n  echo \"started log line not found. Exiting.\"\n  exit 1\nfi\necho \"Required log line found. Broker is ready.\"\nexit 0\n",
        "delay" : 0,
        "interval" : 60,
        "timeout" : 120
	      },
      "config-files" : [ {
&        "name" : "server-properties",
G        "relative-path" : "kafka_1.23-4.5.6/config/server.properties",
¥v        "template-content" : "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id={{POD_INSTANCE_INDEX}}\n\n{{#PLACEMENT_REFERENCED_ZONE}}\n{{#ZONE}}\nbroker.rack={{ZONE}}\n{{/ZONE}}\n{{/PLACEMENT_REFERENCED_ZONE}}\n\n############################# Socket Server Settings #############################\n\n# The address the socket server listens on. It will get the value returned from\n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = security_protocol://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\n#\n# Hostname and port the broker will advertise to producers and consumers. If not set,\n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\n#\n# advertised.listeners=PLAINTEXT://your.host.name:9092\n{{SETUP_HELPER_ADVERTISED_LISTENERS}}\n{{SETUP_HELPER_LISTENERS}}\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n############################# TLS Settings #############################\nssl.keystore.location={{MESOS_SANDBOX}}/broker.keystore\nssl.keystore.password=notsecure\nssl.key.password=notsecure\n\nssl.truststore.location={{MESOS_SANDBOX}}/broker.truststore\nssl.truststore.password=notsecure\n\nssl.enabled.protocols=TLSv1.2\n\n{{#SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\nssl.cipher.suites={{SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_CIPHERS}}\n\n# If Kerberos is NOT enabled, then SSL authentication can be turned on.\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nssl.client.auth=required\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_TRANSPORT_ENCRYPTION_ENABLED}}\n\n{{#SECURITY_KERBEROS_ENABLED}}\n############################# Kerberos Settings #############################\nsasl.mechanism.inter.broker.protocol=GSSAPI\nsasl.enabled.mechanisms=GSSAPI\nsasl.kerberos.service.name={{SECURITY_KERBEROS_PRIMARY}}\n\n{{/SECURITY_KERBEROS_ENABLED}}\n\n## Inter Broker Protocol\n{{SETUP_HELPER_SECURITY_INTER_BROKER_PROTOCOL}}\n\n# The number of threads handling network requests\nnum.network.threads={{KAFKA_NUM_NETWORK_THREADS}}\n\n# The number of threads doing disk I/O\nnum.io.threads={{KAFKA_NUM_IO_THREADS}}\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes={{KAFKA_SOCKET_RECEIVE_BUFFER_BYTES}}\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes={{KAFKA_SOCKET_REQUEST_MAX_BYTES}}\n\n{{#SECURITY_AUTHORIZATION_ENABLED}}\n############################# Authorization Settings #############################\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\nsuper.users={{SETUP_HELPER_SUPER_USERS}}\nallow.everyone.if.no.acl.found={{SECURITY_AUTHORIZATION_ALLOW_EVERYONE_IF_NO_ACL_FOUND}}\n{{^SECURITY_KERBEROS_ENABLED}}\n{{#SECURITY_SSL_AUTHENTICATION_ENABLED}}\nprincipal.builder.class=de.thmshmm.kafka.CustomPrincipalBuilder\n{{/SECURITY_SSL_AUTHENTICATION_ENABLED}}\n{{/SECURITY_KERBEROS_ENABLED}}\n{{/SECURITY_AUTHORIZATION_ENABLED}}\n\n############################# Log Basics #############################\n\n# A comma separated list of directories under which to store log files\nlog.dirs={{KAFKA_DISK_PATH}}/broker-{{POD_INSTANCE_INDEX}}\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions={{KAFKA_NUM_PARTITIONS}}\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir={{KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR}}\n\n############################# Log Flush Policy #############################\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk.\n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.\n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n# The number of messages to accept before forcing a flush of data to disk\nlog.flush.interval.messages={{KAFKA_LOG_FLUSH_INTERVAL_MESSAGES}}\n\n{{#KAFKA_LOG_FLUSH_INTERVAL_MS}}\nlog.flush.interval.ms={{KAFKA_LOG_FLUSH_INTERVAL_MS}}\n{{/KAFKA_LOG_FLUSH_INTERVAL_MS}}\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\n{{#KAFKA_LOG_RETENTION_MS}}\nlog.retention.ms={{KAFKA_LOG_RETENTION_MS}}\n{{/KAFKA_LOG_RETENTION_MS}}\n{{#KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.minutes={{KAFKA_LOG_RETENTION_MINUTES}}\n{{/KAFKA_LOG_RETENTION_MINUTES}}\nlog.retention.hours={{KAFKA_LOG_RETENTION_HOURS}}\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\nlog.retention.bytes={{KAFKA_LOG_RETENTION_BYTES}}\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}\n\n# The interval at which log segments are checked to see if they can be deleted according\n# to the retention policies\nlog.retention.check.interval.ms={{KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS}}\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect={{KAFKA_ZOOKEEPER_URI}}\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n\n########################### Addition Parameters ########################\n\nexternal.kafka.statsd.port={{STATSD_UDP_PORT}}\nexternal.kafka.statsd.host={{STATSD_UDP_HOST}}\nexternal.kafka.statsd.reporter.enabled=true\nexternal.kafka.statsd.tag.enabled=true\nexternal.kafka.statsd.metrics.exclude_regex=\n\nauto.create.topics.enable={{KAFKA_AUTO_CREATE_TOPICS_ENABLE}}\nauto.leader.rebalance.enable={{KAFKA_AUTO_LEADER_REBALANCE_ENABLE}}\n\nbackground.threads={{KAFKA_BACKGROUND_THREADS}}\n\ncompression.type={{KAFKA_COMPRESSION_TYPE}}\n\nconnections.max.idle.ms={{KAFKA_CONNECTIONS_MAX_IDLE_MS}}\n\ncontrolled.shutdown.enable={{KAFKA_CONTROLLED_SHUTDOWN_ENABLE}}\ncontrolled.shutdown.max.retries={{KAFKA_CONTROLLED_SHUTDOWN_MAX_RETRIES}}\ncontrolled.shutdown.retry.backoff.ms={{KAFKA_CONTROLLED_SHUTDOWN_RETRY_BACKOFF_MS}}\ncontroller.socket.timeout.ms={{KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS}}\n\ndefault.replication.factor={{KAFKA_DEFAULT_REPLICATION_FACTOR}}\n\ndelete.topic.enable={{KAFKA_DELETE_TOPIC_ENABLE}}\n\ndelete.records.purgatory.purge.interval.requests={{KAFKA_DELETE_RECORDS_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nfetch.purgatory.purge.interval.requests={{KAFKA_FETCH_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\ngroup.max.session.timeout.ms={{KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS}}\ngroup.min.session.timeout.ms={{KAFKA_GROUP_MIN_SESSION_TIMEOUT_MS}}\ngroup.initial.rebalance.delay.ms={{KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}}\n\ninter.broker.protocol.version={{KAFKA_INTER_BROKER_PROTOCOL_VERSION}}\n\nleader.imbalance.check.interval.seconds={{KAFKA_LEADER_IMBALANCE_CHECK_INTERVAL_SECONDS}}\nleader.imbalance.per.broker.percentage={{KAFKA_LEADER_IMBALANCE_PER_BROKER_PERCENTAGE}}\n\nlog.cleaner.backoff.ms={{KAFKA_LOG_CLEANER_BACKOFF_MS}}\nlog.cleaner.dedupe.buffer.size={{KAFKA_LOG_CLEANER_DEDUPE_BUFFER_SIZE}}\nlog.cleaner.delete.retention.ms={{KAFKA_LOG_CLEANER_DELETE_RETENTION_MS}}\nlog.cleaner.enable={{KAFKA_LOG_CLEANER_ENABLE}}\nlog.cleaner.io.buffer.load.factor={{KAFKA_LOG_CLEANER_IO_BUFFER_LOAD_FACTOR}}\nlog.cleaner.io.buffer.size={{KAFKA_LOG_CLEANER_IO_BUFFER_SIZE}}\nlog.cleaner.io.max.bytes.per.second={{KAFKA_LOG_CLEANER_IO_MAX_BYTES_PER_SECOND}}\nlog.cleaner.min.cleanable.ratio={{KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO}}\nlog.cleaner.min.compaction.lag.ms={{KAFKA_LOG_CLEANER_MIN_COMPACTION_LAG_MS}}\nlog.cleaner.threads={{KAFKA_LOG_CLEANER_THREADS}}\nlog.cleanup.policy={{KAFKA_LOG_CLEANUP_POLICY}}\n\nlog.flush.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_OFFSET_CHECKPOINT_INTERVAL_MS}}\nlog.flush.scheduler.interval.ms={{KAFKA_LOG_FLUSH_SCHEDULER_INTERVAL_MS}}\nlog.flush.start.offset.checkpoint.interval.ms={{KAFKA_LOG_FLUSH_START_OFFSET_CHECKPOINT_INTERVAL_MS}}\n\nlog.index.interval.bytes={{KAFKA_LOG_INDEX_INTERVAL_BYTES}}\nlog.index.size.max.bytes={{KAFKA_LOG_INDEX_SIZE_MAX_BYTES}}\n\nlog.message.format.version={{KAFKA_LOG_MESSAGE_FORMAT_VERSION}}\n\nlog.preallocate={{KAFKA_LOG_PREALLOCATE}}\n\n{{#KAFKA_LOG_ROLL_MS}}\nlog.roll.ms={{KAFKA_LOG_ROLL_MS}}\n{{/KAFKA_LOG_ROLL_MS}}\nlog.roll.hours={{KAFKA_LOG_ROLL_HOURS}}\n{{#KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.ms={{KAFKA_LOG_ROLL_JITTER_MS}}\n{{/KAFKA_LOG_ROLL_JITTER_MS}}\nlog.roll.jitter.hours={{KAFKA_LOG_ROLL_JITTER_HOURS}}\n\nlog.segment.delete.delay.ms={{KAFKA_LOG_SEGMENT_DELETE_DELAY_MS}}\n\nmax.connections={{KAFKA_MAX_CONNECTIONS}}\nmax.connections.per.ip.overrides={{KAFKA_MAX_CONNECTIONS_PER_IP_OVERRIDES}}\nmax.connections.per.ip={{KAFKA_MAX_CONNECTIONS_PER_IP}}\n\nmessage.max.bytes={{KAFKA_MESSAGE_MAX_BYTES}}\n\nkafka.metrics.reporters={{KAFKA_METRICS_REPORTERS}}\nmetric.reporters={{METRIC_REPORTERS}}\nmetrics.num.samples={{KAFKA_METRICS_NUM_SAMPLES}}\nmetrics.sample.window.ms={{KAFKA_METRICS_SAMPLE_WINDOW_MS}}\n\nmin.insync.replicas={{KAFKA_MIN_INSYNC_REPLICAS}}\n\nnum.replica.fetchers={{KAFKA_NUM_REPLICA_FETCHERS}}\n\noffset.metadata.max.bytes={{KAFKA_OFFSET_METADATA_MAX_BYTES}}\noffsets.commit.required.acks={{KAFKA_OFFSETS_COMMIT_REQUIRED_ACKS}}\noffsets.commit.timeout.ms={{KAFKA_OFFSETS_COMMIT_TIMEOUT_MS}}\noffsets.load.buffer.size={{KAFKA_OFFSETS_LOAD_BUFFER_SIZE}}\noffsets.retention.check.interval.ms={{KAFKA_OFFSETS_RETENTION_CHECK_INTERVAL_MS}}\noffsets.retention.minutes={{KAFKA_OFFSETS_RETENTION_MINUTES}}\noffsets.topic.compression.codec={{KAFKA_OFFSETS_TOPIC_COMPRESSION_CODEC}}\noffsets.topic.num.partitions={{KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}}\noffsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}\noffsets.topic.segment.bytes={{KAFKA_OFFSETS_TOPIC_SEGMENT_BYTES}}\n\nproducer.purgatory.purge.interval.requests={{KAFKA_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS}}\n\nqueued.max.requests={{KAFKA_QUEUED_MAX_REQUESTS}}\nqueued.max.request.bytes={{KAFKA_QUEUED_MAX_REQUEST_BYTES}}\nquota.consumer.default={{KAFKA_QUOTA_CONSUMER_DEFAULT}}\nquota.producer.default={{KAFKA_QUOTA_PRODUCER_DEFAULT}}\nquota.window.num={{KAFKA_QUOTA_WINDOW_NUM}}\nquota.window.size.seconds={{KAFKA_QUOTA_WINDOW_SIZE_SECONDS}}\n\nreplica.fetch.backoff.ms={{KAFKA_REPLICA_FETCH_BACKOFF_MS}}\nreplica.fetch.max.bytes={{KAFKA_REPLICA_FETCH_MAX_BYTES}}\nreplica.fetch.min.bytes={{KAFKA_REPLICA_FETCH_MIN_BYTES}}\nreplica.fetch.response.max.bytes={{KAFKA_REPLICA_FETCH_RESPONSE_MAX_BYTES}}\nreplica.fetch.wait.max.ms={{KAFKA_REPLICA_FETCH_WAIT_MAX_MS}}\nreplica.high.watermark.checkpoint.interval.ms={{KAFKA_REPLICA_HIGH_WATERMARK_CHECKPOINT_INTERVAL_MS}}\nreplica.lag.time.max.ms={{KAFKA_REPLICA_LAG_TIME_MAX_MS}}\nreplica.socket.receive.buffer.bytes={{KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES}}\nreplica.socket.timeout.ms={{KAFKA_REPLICA_SOCKET_TIMEOUT_MS}}\n\nreplication.quota.window.num={{KAFKA_REPLICATION_QUOTA_WINDOW_NUM}}\nreplication.quota.window.size.seconds={{KAFKA_REPLICATION_QUOTA_WINDOW_SIZE_SECONDS}}\n\nrequest.timeout.ms={{KAFKA_REQUEST_TIMEOUT_MS}}\n\nreserved.broker.max.id={{KAFKA_RESERVED_BROKER_MAX_ID}}\n\n{{#KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=HTTPS\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n{{^KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\nssl.endpoint.identification.algorithm=\n{{/KAFKA_SSL_ENDPOINT_IDENTIFICATION_ENABLED}}\n\ntransaction.state.log.segment.bytes={{KAFKA_TRANSACTION_STATE_LOG_SEGMENT_BYTES}}\ntransaction.remove.expired.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.max.timeout.ms={{KAFKA_TRANSACTION_MAX_TIMEOUT_MS}}\ntransaction.state.log.num.partitions={{KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS}}\ntransaction.abort.timed.out.transaction.cleanup.interval.ms={{KAFKA_TRANSACTION_ABORT_TIMED_OUT_TRANSACTION_CLEANUP_INTERVAL_MS}}\ntransaction.state.log.load.buffer.size={{KAFKA_TRANSACTION_STATE_LOG_LOAD_BUFFER_SIZE}}\ntransactional.id.expiration.ms={{KAFKA_TRANSACTIONAL_ID_EXPIRATION_MS}}\ntransaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}\ntransaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}\n\nunclean.leader.election.enable={{KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE}}\n\nzookeeper.session.timeout.ms={{KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS}}\nzookeeper.sync.time.ms={{KAFKA_ZOOKEEPER_SYNC_TIME_MS}}\n\n########################################################################\n"
      } ],
      "discovery-spec" : null,
       "kill-grace-period" : 30,
#      "transport-encryption" : [ ]
	    } ],
    "placement-rule" : {
      "@type" : "AndRule",
      "rules" : [ {
&        "@type" : "IsLocalRegionRule"
      }, {
(        "@type" : "MaxPerHostnameRule",
        "max" : 1,
        "task-filter" : {
$          "@type" : "RegexMatcher",
!          "pattern" : "kafka-.*"

        }

      } ]
    },
    "volumes" : [ ],
    "pre-reserved-role" : "*",
    "secrets" : [ ],
#    "share-pid-namespace" : false,
    "host-volumes" : [ ],
"    "seccomp-unconfined" : false,
"    "seccomp-profile-name" : null
  } ]
}
¶INFO  2019-09-05 09:55:45,120 [Test worker] DefaultConfigurationUpdater:updateConfiguration(147): Skipping config diff: There is no old config target to diff against
INFO  2019-09-05 09:55:45,122 [Test worker] DefaultConfigurationUpdater:updateConfiguration(197): Updating target configuration: Prior target configuration 'null' is different from new configuration '63be63de-0da5-40db-b586-105b0d1f712e'. 
≤INFO  2019-09-05 09:55:45,122 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(303): Testing deserialization of 1 listed configurations before cleanup:
öINFO  2019-09-05 09:55:45,122 [Test worker] DefaultConfigurationUpdater:cleanupDuplicateAndUnusedConfigs(308): - 63be63de-0da5-40db-b586-105b0d1f712e: OK
ÖINFO  2019-09-05 09:55:45,122 [Test worker] DefaultConfigurationUpdater:clearConfigsNotListed(413): Cleaning up 0 unused configs: []
ÉINFO  2019-09-05 09:55:45,123 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-0, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,123 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-0-broker at: Tasks/kafka-0-broker/TaskInfo
mINFO  2019-09-05 09:55:45,123 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-0-broker=RUNNING}
öINFO  2019-09-05 09:55:45,123 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,123 [Test worker] DeploymentStep:setStatus(100): kafka-0:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:45,124 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-1, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,124 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-1-broker at: Tasks/kafka-1-broker/TaskInfo
mINFO  2019-09-05 09:55:45,124 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-1-broker=RUNNING}
öINFO  2019-09-05 09:55:45,124 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,124 [Test worker] DeploymentStep:setStatus(100): kafka-1:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
ÉINFO  2019-09-05 09:55:45,125 [Test worker] DefaultStepFactory:getStep(58): Generating step for pod: kafka-2, with tasks: [broker]
¢WARN  2019-09-05 09:55:45,125 [Test worker] StateStore:fetchTask(382): No TaskInfo found for the requested name: kafka-2-broker at: Tasks/kafka-2-broker/TaskInfo
mINFO  2019-09-05 09:55:45,125 [Test worker] DeploymentStep:<init>(73): Goal states: {kafka-2-broker=RUNNING}
öINFO  2019-09-05 09:55:45,125 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
öINFO  2019-09-05 09:55:45,125 [Test worker] DeploymentStep:setStatus(100): kafka-2:[broker]: changed status from: PENDING to: PENDING (interrupted=false)
fINFO  2019-09-05 09:55:45,125 [Test worker] SchedulerBuilder:getPlans(678): Got 1 YAML plan: [deploy]
êINFO  2019-09-05 09:55:45,126 [Test worker] SchedulerBuilder:selectDeployPlan(696): Using regular deploy plan: No custom update plan is defined
nINFO  2019-09-05 09:55:45,126 [Test worker] SchedulerBuilder:getDefaultScheduler(553): Plan: deploy (PENDING)
  Phase: broker (PENDING)
%    Step: kafka-0:[broker] (PENDING)
%    Step: kafka-1:[broker] (PENDING)
%    Step: kafka-2:[broker] (PENDING)
INFO  2019-09-05 09:55:45,126 [Test worker] DecommissionPlanFactory:getPodsToDecommission(195): Expected pod counts: {kafka=3}
ÑINFO  2019-09-05 09:55:45,127 [Test worker] DecommissionPlanFactory:getPodsToDecommission(228): Pods scheduled for decommission: []
úINFO  2019-09-05 09:55:45,127 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 5s
úINFO  2019-09-05 09:55:45,128 [Test worker] TokenBucket:<init>(59): Configured with count: 256, capacity: 256, incrementInterval: 256s, acquireInterval: 0s
